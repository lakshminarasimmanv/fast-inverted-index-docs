"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[1864],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(6540);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}},9365:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"examples","title":"examples","description":"This section provides practical examples of using Fast-Inverted-Index for real-world scenarios.","source":"@site/docs/examples.md","sourceDirName":".","slug":"/examples","permalink":"/docs/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/username/fast-inverted-index/tree/main/docusaurus/docs/examples.md","tags":[],"version":"current","frontMatter":{"id":"examples","title":"examples","sidebar_label":"examples"},"sidebar":"docs","previous":{"title":"query uuilder","permalink":"/docs/query-builder"},"next":{"title":"architecture","permalink":"/docs/architecture"}}');var i=t(4848),a=t(8453);const s={id:"examples",title:"examples",sidebar_label:"examples"},o="Usage Examples",d={},c=[{value:"Basic Search Engine",id:"basic-search-engine",level:2},{value:"Wikipedia Indexer",id:"wikipedia-indexer",level:2},{value:"Advanced Query Construction",id:"advanced-query-construction",level:2},{value:"Real-time Search with Indexing Updates",id:"real-time-search-with-indexing-updates",level:2},{value:"Benchmarking Performance",id:"benchmarking-performance",level:2},{value:"Advanced Query Engine Examples",id:"advanced-query-engine-examples",level:2},{value:"Multi-Field Search with Scoring Explanation",id:"multi-field-search-with-scoring-explanation",level:3},{value:"Filtering with Range Queries",id:"filtering-with-range-queries",level:3},{value:"Fuzzy Search and Prefix Queries",id:"fuzzy-search-and-prefix-queries",level:3}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"usage-examples",children:"Usage Examples"})}),"\n",(0,i.jsx)(n.p,{children:"This section provides practical examples of using Fast-Inverted-Index for real-world scenarios."}),"\n",(0,i.jsx)(n.h2,{id:"basic-search-engine",children:"Basic Search Engine"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete search engine with indexing and querying capabilities:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index\nimport os\n\nclass SimpleSearchEngine:\n    def __init__(self, storage_path=None, in_memory=False):\n        """Initialize the search engine."""\n        self.index = Index(\n            storage_path=storage_path,\n            in_memory=in_memory,\n            cache_size=10000,\n        )\n        self.next_doc_id = 1\n\n    def index_file(self, file_path):\n        """Index a text file."""\n        doc_id = self.next_doc_id\n        self.next_doc_id += 1\n\n        with open(file_path, "r", encoding="utf-8") as f:\n            content = f.read()\n\n        # Extract metadata from the file\n        metadata = {\n            "title": os.path.basename(file_path),\n            "source_url": file_path,\n            "language": "en",\n        }\n\n        # Add the document to the index\n        self.index.add_document(\n            doc_id=doc_id,\n            content=content,\n            metadata=metadata,\n        )\n\n        return doc_id\n\n    def index_directory(self, directory_path, file_pattern=r".*\\.txt$"):\n        """Index all text files in a directory."""\n        import re\n        doc_ids = []\n        file_regex = re.compile(file_pattern)\n\n        for root, _, files in os.walk(directory_path):\n            for file in files:\n                if file_regex.match(file):\n                    file_path = os.path.join(root, file)\n                    doc_id = self.index_file(file_path)\n                    doc_ids.append(doc_id)\n\n        return doc_ids\n\n    def search(self, query):\n        """Search the index."""\n        # Split the query into words\n        words = query.lower().split()\n        \n        try:\n            # For single term queries\n            if len(words) == 1:\n                doc_ids = self.index.term_query(words[0])\n            # For multi-term queries - use AND by default\n            else:\n                doc_ids = self.index.and_query(words)\n            \n            # Get documents for the matching IDs\n            results = []\n            for doc_id in doc_ids:\n                try:\n                    doc = self.index.get_document(doc_id)\n                    results.append(doc)\n                except Exception:\n                    pass\n                    \n            return results\n        except Exception as e:\n            print(f"Search failed: {e}")\n            return []\n\n    def suggest(self, prefix):\n        """Get term suggestions."""\n        return self.index.suggest_terms(prefix)\n\n    def stats(self):\n        """Get index statistics."""\n        return self.index.stats()\n\n    def optimize(self):\n        """Optimize the index."""\n        self.index.optimize()\n\n\n# Usage example\nif __name__ == "__main__":\n    import sys\n    \n    # Create the search engine\n    engine = SimpleSearchEngine(in_memory=True)\n    \n    # Check if a directory was provided\n    if len(sys.argv) > 1:\n        directory = sys.argv[1]\n        print(f"Indexing directory: {directory}")\n        doc_ids = engine.index_directory(directory)\n        print(f"Indexed {len(doc_ids)} documents")\n    \n    # Interactive search loop\n    print("Enter a query (or \'quit\' to exit):")\n    while True:\n        query = input("> ")\n        if query.lower() == "quit":\n            break\n        elif query.lower() == "stats":\n            stats = engine.stats()\n            for key, value in stats.items():\n                print(f"{key}: {value}")\n        elif query.lower().startswith("suggest "):\n            prefix = query[8:].strip()\n            suggestions = engine.suggest(prefix)\n            for suggestion in suggestions:\n                print(f"{suggestion[\'term\']} (score: {suggestion[\'score\']})")\n        else:\n            results = engine.search(query)\n            print(f"Found {len(results)} results:")\n            for i, doc in enumerate(results[:10], 1):\n                print(f"{i}. {doc[\'title\']} (ID: {doc[\'id\']})")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"wikipedia-indexer",children:"Wikipedia Indexer"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates indexing Wikipedia articles:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport time\n\nclass WikipediaIndexer:\n    def __init__(self, storage_path=None, in_memory=False):\n        """Initialize the indexer."""\n        self.index = Index(\n            storage_path=storage_path,\n            in_memory=in_memory,\n            cache_size=100000,\n            cache_ttl_secs=3600,\n        )\n        self.next_doc_id = 1\n\n    def index_article(self, article):\n        """Index a Wikipedia article."""\n        doc_id = self.next_doc_id\n        self.next_doc_id += 1\n\n        # Extract article metadata\n        title = article.get("title", "")\n        text = article.get("text", "")\n        url = article.get("url", "")\n        categories = article.get("categories", [])\n\n        # Clean the text\n        if isinstance(text, str):\n            # Remove HTML tags\n            soup = BeautifulSoup(text, "html.parser")\n            text = soup.get_text()\n        else:\n            text = ""\n\n        # Add the document to the index\n        self.index.add_document(\n            doc_id=doc_id,\n            content=text,\n            metadata={\n                "title": title,\n                "source_url": url,\n                "category": ", ".join(categories) if categories else None,\n                "language": "en",\n                "tags": categories if categories else [],\n            },\n        )\n\n        return doc_id\n\n    def search(self, query, limit=10):\n        """Search the index."""\n        # Split the query into words\n        words = query.lower().split()\n        \n        try:\n            # For single term queries\n            if len(words) == 1:\n                doc_ids = self.index.term_query(words[0])\n            # For multi-term queries - use AND by default\n            else:\n                doc_ids = self.index.and_query(words)\n            \n            # Limit the number of results\n            if limit > 0:\n                doc_ids = doc_ids[:limit]\n            \n            # Get documents for the matching IDs\n            results = []\n            for doc_id in doc_ids:\n                try:\n                    doc = self.index.get_document(doc_id)\n                    results.append(doc)\n                except Exception:\n                    pass\n                    \n            return results\n        except Exception as e:\n            print(f"Search failed: {e}")\n            return []\n\n    def download_sample_articles(self, count=100):\n        """Download sample Wikipedia articles."""\n        articles = []\n        batch_size = min(count, 20)  # Wikipedia API limits\n        continue_params = {}\n\n        while len(articles) < count:\n            # Prepare the API request\n            params = {\n                "action": "query",\n                "format": "json",\n                "list": "random",\n                "rnnamespace": 0,  # Main namespace\n                "rnlimit": batch_size,\n            }\n            params.update(continue_params)\n\n            # Make the API request\n            response = requests.get(\n                "https://en.wikipedia.org/w/api.php",\n                params=params,\n                headers={"User-Agent": "WikipediaIndexerExample/1.0"},\n            )\n            data = response.json()\n\n            # Process the results\n            for page in data.get("query", {}).get("random", []):\n                page_id = page.get("id")\n                title = page.get("title")\n\n                # Get the article content\n                content_params = {\n                    "action": "query",\n                    "format": "json",\n                    "prop": "extracts|categories|info",\n                    "exintro": 1,\n                    "explaintext": 1,\n                    "inprop": "url",\n                    "pageids": page_id,\n                }\n\n                content_response = requests.get(\n                    "https://en.wikipedia.org/w/api.php",\n                    params=content_params,\n                    headers={"User-Agent": "WikipediaIndexerExample/1.0"},\n                )\n                content_data = content_response.json()\n\n                # Extract the article data\n                page_data = content_data.get("query", {}).get("pages", {}).get(str(page_id), {})\n                extract = page_data.get("extract", "")\n                url = page_data.get("fullurl", "")\n                categories = [\n                    cat.get("title", "").replace("Category:", "")\n                    for cat in page_data.get("categories", [])\n                ]\n\n                # Add the article to the list\n                articles.append({\n                    "id": page_id,\n                    "title": title,\n                    "text": extract,\n                    "url": url,\n                    "categories": categories,\n                })\n\n                if len(articles) >= count:\n                    break\n\n            # Check if there are more results\n            if "continue" not in data:\n                break\n            continue_params = data.get("continue", {})\n\n        return articles\n\n\n# Usage example\nif __name__ == "__main__":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description="Wikipedia indexer")\n    parser.add_argument("--download", type=int, default=10,\n                      help="Number of articles to download")\n    parser.add_argument("--storage", help="Path to store the index")\n    parser.add_argument("--in-memory", action="store_true", \n                      help="Use in-memory mode")\n    args = parser.parse_args()\n    \n    # Create the indexer\n    indexer = WikipediaIndexer(storage_path=args.storage, \n                             in_memory=args.in_memory)\n    \n    # Download and index articles\n    print(f"Downloading {args.download} articles...")\n    start_time = time.time()\n    articles = indexer.download_sample_articles(args.download)\n    for article in articles:\n        indexer.index_article(article)\n    elapsed_time = time.time() - start_time\n    print(f"Indexed {len(articles)} articles in {elapsed_time:.2f} seconds")\n    \n    # Interactive search loop\n    print("Enter a query (or \'quit\' to exit):")\n    while True:\n        query = input("> ")\n        if query.lower() == "quit":\n            break\n        elif query.lower() == "stats":\n            stats = indexer.index.stats()\n            for key, value in stats.items():\n                print(f"{key}: {value}")\n        elif query.lower().startswith("suggest "):\n            prefix = query[8:].strip()\n            suggestions = indexer.index.suggest_terms(prefix)\n            for suggestion in suggestions:\n                print(f"{suggestion[\'term\']} (score: {suggestion[\'score\']})")\n        else:\n            results = indexer.search(query)\n            print(f"Found {len(results)} results:")\n            for i, doc in enumerate(results, 1):\n                print(f"{i}. {doc[\'title\']}")\n                if doc.get(\'category\'):\n                    print(f"   Category: {doc[\'category\']}")\n                print(f"   URL: {doc[\'source_url\']}")\n                print()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-query-construction",children:"Advanced Query Construction"}),"\n",(0,i.jsx)(n.p,{children:"This example shows how to build advanced queries with field boosting for better relevance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index\nimport time\n\n# Create an index\nindex = Index(in_memory=True)\n\n# Add documents with rich metadata\nindex.add_document(\n    doc_id=1,\n    content="Python is a high-level programming language known for its readability.",\n    metadata={\n        "title": "Introduction to Python",\n        "author": "John Smith",\n        "tags": ["programming", "python", "beginner"],\n        "category": "Programming Languages",\n    }\n)\n\nindex.add_document(\n    doc_id=2,\n    content="Rust is a systems programming language focused on safety and performance.",\n    metadata={\n        "title": "Rust Programming Language",\n        "author": "Jane Doe",\n        "tags": ["programming", "rust", "systems"],\n        "category": "Programming Languages",\n    }\n)\n\nindex.add_document(\n    doc_id=3,\n    content="Search engines index documents to provide fast query results.",\n    metadata={\n        "title": "How Search Engines Work",\n        "author": "John Smith",\n        "tags": ["search", "indexing", "algorithms"],\n        "category": "Information Retrieval",\n    }\n)\n\n# Basic search\nprint("Basic search for \'programming\':")\nresults = index.term_query("programming")\nprint(f"Found {len(results)} results")\n\n# AND query\nprint("\\nAND query for \'programming\' AND \'language\':")\nresults = index.and_query(["programming", "language"])\nprint(f"Found {len(results)} results")\n\n# OR query\nprint("\\nOR query for \'python\' OR \'rust\':")\nresults = index.or_query(["python", "rust"])\nprint(f"Found {len(results)} results")\n\n# Parsed query\nprint("\\nParsed query for \'programming language\':")\nresults = index.parse_query("programming language")\nprint(f"Found {len(results)} results")\n\n# Ranked search with BM25\nprint("\\nRanked search with BM25:")\nresults = index.search("programming language", ranking_method="bm25")\nfor doc_id, score in results:\n    doc = index.get_document(doc_id)\n    print(f"- {doc[\'title\']} (Score: {score:.4f})")\n\n# Field boosting\nprint("\\nRanked search with field boosting:")\nresults = index.search_with_metadata(\n    "programming",\n    ranking_method="bm25",\n    boost_fields={"title": 2.0, "tags": 1.5}\n)\nfor doc in results:\n    print(f"- {doc[\'title\']} (Score: {doc[\'score\']:.4f})")\n\n# Term suggestions\nprint("\\nTerm suggestions for \'pro\':")\nsuggestions = index.suggest_terms("pro")\nfor suggestion in suggestions:\n    print(f"- {suggestion[\'term\']} (Score: {suggestion[\'score\']:.4f})")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"real-time-search-with-indexing-updates",children:"Real-time Search with Indexing Updates"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates how to build a search engine that updates its index in real-time:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import threading\nimport time\nimport random\nfrom fast_inverted_index import Index\n\nclass RealtimeSearchEngine:\n    def __init__(self, storage_path=None):\n        """Initialize the real-time search engine."""\n        self.index = Index(storage_path=storage_path)\n        self.lock = threading.Lock()\n        self.next_doc_id = 1\n        self.stop_requested = False\n        self.indexer_thread = None\n        \n    def start_indexer(self, interval=1.0):\n        """Start the background indexer thread."""\n        if self.indexer_thread is not None:\n            return\n            \n        self.stop_requested = False\n        self.indexer_thread = threading.Thread(\n            target=self._indexer_loop,\n            args=(interval,)\n        )\n        self.indexer_thread.daemon = True\n        self.indexer_thread.start()\n        \n    def stop_indexer(self):\n        """Stop the background indexer thread."""\n        self.stop_requested = True\n        if self.indexer_thread:\n            self.indexer_thread.join()\n            self.indexer_thread = None\n            \n    def _indexer_loop(self, interval):\n        """Background thread that simulates incoming documents."""\n        while not self.stop_requested:\n            # Simulate a new document arriving\n            self.add_document(self._generate_random_document())\n            time.sleep(interval)\n            \n    def _generate_random_document(self):\n        """Generate a random document for testing."""\n        topics = ["Python", "Rust", "Java", "JavaScript", "Go", "C++", "Ruby"]\n        actions = ["Programming", "Development", "Coding", "Learning", "Teaching"]\n        qualities = ["Fast", "Efficient", "Modern", "Reliable", "Scalable"]\n        \n        topic = random.choice(topics)\n        action = random.choice(actions)\n        quality = random.choice(qualities)\n        \n        return {\n            "content": f"{topic} is great for {action}. It\'s a {quality} language.",\n            "metadata": {\n                "title": f"{topic} for {action}",\n                "tags": [topic.lower(), action.lower(), quality.lower()],\n                "category": "Programming Languages",\n                "timestamp": time.time()\n            }\n        }\n        \n    def add_document(self, doc_data):\n        """Add a document to the index."""\n        with self.lock:\n            doc_id = self.next_doc_id\n            self.next_doc_id += 1\n            \n            self.index.add_document(\n                doc_id=doc_id,\n                content=doc_data["content"],\n                metadata=doc_data["metadata"]\n            )\n            return doc_id\n            \n    def search(self, query):\n        """Search the index with the given query."""\n        with self.lock:\n            # Use the search_with_metadata method for rich results\n            return self.index.search_with_metadata(\n                query, \n                ranking_method="bm25",\n                limit=10\n            )\n            \n    def stats(self):\n        """Get current statistics."""\n        with self.lock:\n            return self.index.stats()\n\n\n# Usage example\nif __name__ == "__main__":\n    # Create the search engine\n    search_engine = RealtimeSearchEngine()\n    \n    # Start the background indexer\n    search_engine.start_indexer(interval=0.5)  # Add a document every 0.5 seconds\n    \n    try:\n        # Main loop for searching\n        print("Real-time search engine started. Enter queries or \'quit\' to exit.")\n        print("Documents are being added in the background...")\n        \n        while True:\n            query = input("> ")\n            if query.lower() == \'quit\':\n                break\n            elif query.lower() == \'stats\':\n                stats = search_engine.stats()\n                print(f"Documents: {stats[\'document_count\']}")\n                print(f"Terms: {stats[\'term_count\']}")\n                print(f"Cache hit rate: {stats[\'cache_hit_rate\']}")\n            else:\n                start_time = time.time()\n                results = search_engine.search(query)\n                elapsed = time.time() - start_time\n                \n                print(f"Found {len(results)} results in {elapsed:.4f} seconds:")\n                for i, doc in enumerate(results, 1):\n                    print(f"{i}. {doc[\'title\']} (Score: {doc[\'score\']:.4f})")\n                    print(f"   Tags: {\', \'.join(doc[\'tags\'])}")\n                    \n    finally:\n        # Clean up\n        search_engine.stop_indexer()\n        print("Search engine stopped.")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"benchmarking-performance",children:"Benchmarking Performance"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates how to benchmark indexing and query performance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index\nimport time\nimport random\nimport string\nimport multiprocessing\nimport statistics\n\ndef generate_random_text(word_count, max_word_length=10):\n    """Generate random text with the specified number of words."""\n    words = []\n    for _ in range(word_count):\n        word_length = random.randint(3, max_word_length)\n        word = \'\'.join(random.choice(string.ascii_lowercase) for _ in range(word_length))\n        words.append(word)\n    return \' \'.join(words)\n\ndef generate_random_documents(count, min_words=50, max_words=200):\n    """Generate a list of random documents."""\n    documents = []\n    for i in range(count):\n        word_count = random.randint(min_words, max_words)\n        documents.append({\n            "id": i + 1,\n            "content": generate_random_text(word_count)\n        })\n    return documents\n\ndef benchmark_indexing(index, documents):\n    """Benchmark document indexing performance."""\n    start_time = time.time()\n    for doc in documents:\n        index.add_document(doc["id"], doc["content"])\n    end_time = time.time()\n    \n    total_time = end_time - start_time\n    docs_per_second = len(documents) / total_time\n    \n    return {\n        "total_time": total_time,\n        "docs_per_second": docs_per_second,\n        "avg_time_per_doc": total_time / len(documents)\n    }\n\ndef benchmark_querying(index, queries, repetitions=5):\n    """Benchmark query performance."""\n    results = []\n    \n    for query in queries:\n        query_times = []\n        \n        for _ in range(repetitions):\n            start_time = time.time()\n            index.term_query(query)\n            end_time = time.time()\n            query_times.append(end_time - start_time)\n        \n        avg_time = statistics.mean(query_times)\n        results.append({\n            "query": query,\n            "avg_time": avg_time,\n            "min_time": min(query_times),\n            "max_time": max(query_times),\n            "stddev": statistics.stdev(query_times) if len(query_times) > 1 else 0\n        })\n    \n    return results\n\ndef extract_queries_from_documents(documents, count=100):\n    """Extract random words from documents to use as queries."""\n    all_words = set()\n    for doc in documents:\n        words = doc["content"].split()\n        all_words.update(words)\n    \n    return random.sample(list(all_words), min(count, len(all_words)))\n\ndef parallel_indexing_benchmark(processes, docs_per_process, min_words, max_words):\n    """Benchmark indexing with multiple processes."""\n    results = []\n    \n    def worker(docs_count, min_w, max_w):\n        index = Index(in_memory=True)\n        docs = generate_random_documents(docs_count, min_w, max_w)\n        result = benchmark_indexing(index, docs)\n        return result\n    \n    with multiprocessing.Pool(processes) as pool:\n        args = [(docs_per_process, min_words, max_words) for _ in range(processes)]\n        results = pool.starmap(worker, args)\n    \n    # Aggregate results\n    total_docs = processes * docs_per_process\n    total_time = sum(r["total_time"] for r in results)\n    avg_time_per_doc = statistics.mean(r["avg_time_per_doc"] for r in results)\n    total_docs_per_second = total_docs / total_time\n    \n    return {\n        "processes": processes,\n        "total_docs": total_docs,\n        "total_time": total_time,\n        "avg_time_per_doc": avg_time_per_doc,\n        "total_docs_per_second": total_docs_per_second\n    }\n\n# Run the benchmarks\nif __name__ == "__main__":\n    # Parameters\n    doc_count = 10000\n    query_count = 100\n    \n    # Create the index\n    print(f"Creating index and generating {doc_count} random documents...")\n    index = Index(in_memory=True)\n    documents = generate_random_documents(doc_count)\n    \n    # Benchmark indexing\n    print("Benchmarking indexing performance...")\n    indexing_results = benchmark_indexing(index, documents)\n    print(f"Indexed {doc_count} documents in {indexing_results[\'total_time\']:.2f} seconds")\n    print(f"Indexing speed: {indexing_results[\'docs_per_second\']:.2f} docs/second")\n    \n    # Get index statistics\n    stats = index.stats()\n    print("\\nIndex statistics:")\n    print(f"Documents: {stats[\'document_count\']}")\n    print(f"Terms: {stats[\'term_count\']}")\n    print(f"Average document length: {stats[\'avg_doc_length\']:.2f} tokens")\n    \n    # Benchmark querying\n    print("\\nBenchmarking query performance...")\n    queries = extract_queries_from_documents(documents, query_count)\n    query_results = benchmark_querying(index, queries)\n    \n    avg_query_time = statistics.mean(r["avg_time"] for r in query_results)\n    print(f"Average query time: {avg_query_time * 1000:.2f} ms")\n    print(f"Queries per second: {1.0 / avg_query_time:.2f}")\n    \n    # Benchmark parallel indexing\n    print("\\nBenchmarking parallel indexing...")\n    cpu_count = multiprocessing.cpu_count()\n    docs_per_process = 1000\n    \n    for num_processes in [1, 2, 4, max(1, cpu_count-1)]:\n        print(f"\\nTesting with {num_processes} processes...")\n        parallel_results = parallel_indexing_benchmark(\n            num_processes, docs_per_process, 50, 200)\n        print(f"Total docs: {parallel_results[\'total_docs\']}")\n        print(f"Total time: {parallel_results[\'total_time\']:.2f} seconds")\n        print(f"Throughput: {parallel_results[\'total_docs_per_second\']:.2f} docs/second")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-query-engine-examples",children:"Advanced Query Engine Examples"}),"\n",(0,i.jsx)(n.p,{children:"This section shows how to use the advanced Query Engine API for more complex search requirements."}),"\n",(0,i.jsx)(n.h3,{id:"multi-field-search-with-scoring-explanation",children:"Multi-Field Search with Scoring Explanation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index, PyQueryNode, PyQueryExecutionParams\n\n# Create an index\nindex = Index(in_memory=True)\n\n# Add documents\nindex.add_document(\n    doc_id=1,\n    content="TensorFlow is a popular machine learning framework for deep learning",\n    metadata={\n        "title": "Introduction to TensorFlow",\n        "tags": ["machine learning", "deep learning", "tensorflow"],\n        "category": "Artificial Intelligence"\n    }\n)\n\nindex.add_document(\n    doc_id=2,\n    content="PyTorch is a flexible deep learning framework with dynamic computation graphs",\n    metadata={\n        "title": "PyTorch for Deep Learning",\n        "tags": ["machine learning", "deep learning", "pytorch"],\n        "category": "Artificial Intelligence"\n    }\n)\n\nindex.add_document(\n    doc_id=3,\n    content="Scikit-learn is a traditional machine learning library with many algorithms",\n    metadata={\n        "title": "Machine Learning with Scikit-learn",\n        "tags": ["machine learning", "scikit-learn", "algorithms"],\n        "category": "Data Science"\n    }\n)\n\n# Create a complex query that searches for "machine learning" in different fields\ntitle_query = PyQueryNode.term("title", "machine", boost=2.0)\ncontent_query = PyQueryNode.phrase("content", ["machine", "learning"], slop=1)\ntag_query = PyQueryNode.term("tags", "machine learning", boost=1.5)\n\n# Combine the queries\ncombined_query = PyQueryNode.or([title_query, content_query, tag_query])\n\n# Create execution parameters\nparams = PyQueryExecutionParams(\n    scoring_algorithm="bm25l",\n    explain=True,\n    limit=10,\n    field_boosts={"title": 2.0, "tags": 1.5}\n)\n\n# Execute the query\nresult = index.execute_query(combined_query, params)\n\n# Display results with explanations\nprint(f"Found {result.len()} results in {result.time_ms:.2f}ms\\n")\n\nfor doc_id, score in result.scored_docs:\n    doc = index.get_document(doc_id)\n    print(f"Document: {doc[\'title\']}")\n    print(f"Score: {score:.4f}")\n    print(f"Category: {doc[\'category\']}")\n    print(f"Tags: {\', \'.join(doc[\'tags\'])}")\n    \n    if result.explanations and doc_id in result.explanations:\n        print("\\nScore Explanation:")\n        print(result.explanations[doc_id].to_tree())\n    \n    print("\\n" + "-"*60)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"filtering-with-range-queries",children:"Filtering with Range Queries"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index, PyQueryNode, PyValue, PyQueryBound\n\n# Create an index with technical articles\nindex = Index(in_memory=True)\n\n# Add sample documents with different reading times\nfor i in range(1, 11):\n    # Vary document lengths/complexity\n    reading_time = 5 * i  # 5 to 50 minutes\n    word_count = reading_time * 250  # 1250 to 12500 words\n    \n    # Even IDs are beginner-friendly, odd are advanced\n    level = "Beginner" if i % 2 == 0 else "Advanced"\n    \n    # Create content based on the technical level\n    if level == "Beginner":\n        content = f"This is a beginner-friendly article about programming. It covers basic concepts in {word_count} words."\n    else:\n        content = f"This advanced technical article explores complex programming patterns and algorithms in {word_count} words."\n    \n    # Add the document\n    index.add_document(\n        doc_id=i,\n        content=content,\n        metadata={\n            "title": f"Technical Article {i}",\n            "level": level,\n            "metadata": {\n                "reading_time": reading_time,\n                "word_count": word_count\n            }\n        }\n    )\n\n# Create a query for beginner-friendly articles with 10-20 minute reading time\n# 1. Level filter\nlevel_query = PyQueryNode.term("level", "Beginner")\n\n# 2. Reading time range (10-20 minutes)\nmin_time = PyQueryBound.included(PyValue.integer(10))\nmax_time = PyQueryBound.excluded(PyValue.integer(21))\n# Note: Use direct field name "reading_time", not "metadata.reading_time"\ntime_query = PyQueryNode.range("reading_time", min_time, max_time)\n\n# 3. Content relevance\ncontent_query = PyQueryNode.term("content", "programming")\n\n# Combine: content query with filters for level and reading time\nfinal_query = PyQueryNode.filter(\n    content_query,\n    PyQueryNode.and([level_query, time_query])\n)\n\n# Execute the query\nresult = index.execute_query(final_query)\n\n# Display results\nprint("Beginner-friendly programming articles with 10-20 minute reading time:\\n")\nfor doc_id, score in result.scored_docs:\n    doc = index.get_document(doc_id)\n    print(f"Article: {doc[\'title\']}")\n    print(f"Level: {doc[\'level\']}")\n    print(f"Reading time: {doc[\'metadata\'][\'reading_time\']} minutes")\n    print(f"Word count: {doc[\'metadata\'][\'word_count\']} words")\n    print(f"Score: {score:.4f}")\n    print()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"fuzzy-search-and-prefix-queries",children:"Fuzzy Search and Prefix Queries"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fast_inverted_index import Index, PyQueryNode\n\n# Create an index for a product catalog\nindex = Index(in_memory=True)\n\n# Add sample products\nproducts = [\n    {"id": 1, "name": "Smartphone", "category": "Electronics", \n     "description": "Latest smartphone with high-resolution camera and fast processor."},\n    {"id": 2, "name": "Smart TV", "category": "Electronics", \n     "description": "4K smart television with streaming apps and voice control."},\n    {"id": 3, "name": "Smartwatch", "category": "Wearables", \n     "description": "Fitness tracking smartwatch with heart rate monitoring."},\n    {"id": 4, "name": "Wireless Headphones", "category": "Audio", \n     "description": "Bluetooth headphones with noise cancellation."},\n    {"id": 5, "name": "Bluetooth Speaker", "category": "Audio", \n     "description": "Portable wireless speaker with 20-hour battery life."},\n    {"id": 6, "name": "Digital Camera", "category": "Photography", \n     "description": "High-end digital camera with interchangeable lenses."},\n    {"id": 7, "name": "Laptop", "category": "Computers", \n     "description": "Powerful laptop for gaming and professional work."},\n    {"id": 8, "name": "Tablet", "category": "Electronics", \n     "description": "Lightweight tablet with long battery life and stylus support."},\n    {"id": 9, "name": "Fitness Tracker", "category": "Wearables", \n     "description": "Simple fitness band that tracks steps and sleep."},\n    {"id": 10, "name": "Smart Home Hub", "category": "Smart Home", \n     "description": "Central hub to control all your smart home devices."}\n]\n\n# Add products to the index\nfor product in products:\n    index.add_document(\n        doc_id=product["id"],\n        content=product["description"],\n        metadata={\n            "title": product["name"],\n            "category": product["category"]\n        }\n    )\n\n# 1. Fuzzy search for potentially misspelled terms\nfuzzy_query = PyQueryNode.fuzzy("content", "headphne", max_distance=2)  # Misspelled headphone\nprint("Fuzzy search for \'headphne\' (misspelled headphone):")\nresult = index.execute_query(fuzzy_query)\nfor doc_id, score in result.scored_docs:\n    doc = index.get_document(doc_id)\n    print(f"- {doc[\'title\']} (Score: {score:.4f})")\nprint()\n\n# 2. Prefix search for auto-completion\nprefix_query = PyQueryNode.prefix("title", "Smart")\nprint("Prefix search for products starting with \'Smart\':")\nresult = index.execute_query(prefix_query)\nfor doc_id, score in result.scored_docs:\n    doc = index.get_document(doc_id)\n    print(f"- {doc[\'title\']} (Score: {score:.4f})")\nprint()\n\n# 3. Combined query: products in "Wearables" category with "tracking" in description\ncategory_filter = PyQueryNode.term("category", "Wearables")\ntracking_query = PyQueryNode.term("content", "tracking")\ncombined_query = PyQueryNode.filter(tracking_query, category_filter)\n\nprint("Wearable products with \'tracking\' in the description:")\nresult = index.execute_query(combined_query)\nfor doc_id, score in result.scored_docs:\n    doc = index.get_document(doc_id)\n    print(f"- {doc[\'title\']}")\n    print(f"  Category: {doc[\'category\']}")\n    print(f"  Description: {index.get_document(doc_id)[\'content\']}")\n    print(f"  Score: {score:.4f}")\n'})}),"\n",(0,i.jsx)(n.p,{children:"These examples demonstrate how to use fast-inverted-index for a variety of real-world scenarios. You can adapt them to your specific requirements or use them as starting points for your own search applications."}),"\n",(0,i.jsxs)(n.p,{children:["For more specialized use cases or performance optimizations, refer to the ",(0,i.jsx)(n.a,{href:"/docs/api",children:"API Reference"}),", ",(0,i.jsx)(n.a,{href:"/docs/query-engine",children:"Query Engine"}),", and ",(0,i.jsx)(n.a,{href:"/docs/performance",children:"Performance Tuning"})," sections."]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);