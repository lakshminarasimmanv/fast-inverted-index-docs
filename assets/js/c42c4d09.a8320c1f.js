"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[7298],{7420:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>u,frontMatter:()=>c,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"indexing_throughput_and_content_storage","title":"Maximizing Indexing Throughput with Content Storage Integration","description":"This guide provides comprehensive strategies and practical examples for achieving maximum indexing throughput with Fast Inverted Index while efficiently integrating with external content storage systems. It covers optimizations for index creation, content storage integration, and system-level performance tuning.","source":"@site/docs/indexing_throughput_and_content_storage.md","sourceDirName":".","slug":"/indexing_throughput_and_content_storage","permalink":"/docs/indexing_throughput_and_content_storage","draft":false,"unlisted":false,"editUrl":"https://github.com/username/fast-inverted-index/tree/main/docusaurus/docs/indexing_throughput_and_content_storage.md","tags":[],"version":"current","frontMatter":{"id":"indexing_throughput_and_content_storage","title":"Maximizing Indexing Throughput with Content Storage Integration","sidebar_label":"Throughput & Content Storage"}}');var s=t(4848),o=t(8453);const c={id:"indexing_throughput_and_content_storage",title:"Maximizing Indexing Throughput with Content Storage Integration",sidebar_label:"Throughput & Content Storage"},r="Maximizing Indexing Throughput with Content Storage Integration",a={},d=[{value:"Overview",id:"overview",level:2},{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Throughput Optimization Principles",id:"throughput-optimization-principles",level:2},{value:"Key Performance Factors",id:"key-performance-factors",level:3},{value:"Metrics to Monitor",id:"metrics-to-monitor",level:3},{value:"RocksDB Optimization for Maximum Indexing Speed",id:"rocksdb-optimization-for-maximum-indexing-speed",level:2},{value:"Bulk Loading Configuration",id:"bulk-loading-configuration",level:3},{value:"Enabling Bulk Loading Mode",id:"enabling-bulk-loading-mode",level:3},{value:"WriteBatch Optimization",id:"writebatch-optimization",level:3},{value:"Content Storage Selection and Integration",id:"content-storage-selection-and-integration",level:2},{value:"Selecting the Right Content Storage",id:"selecting-the-right-content-storage",level:3},{value:"Content Store Integration Patterns",id:"content-store-integration-patterns",level:3},{value:"Direct Integration",id:"direct-integration",level:4},{value:"Pipeline Integration",id:"pipeline-integration",level:4},{value:"Batched Processing Patterns",id:"batched-processing-patterns",level:2},{value:"Batch Document Addition",id:"batch-document-addition",level:3},{value:"Optimized Batch Content Retrieval",id:"optimized-batch-content-retrieval",level:3},{value:"Multi-stage Batching",id:"multi-stage-batching",level:3},{value:"Parallel Indexing Strategies",id:"parallel-indexing-strategies",level:2},{value:"Thread Pool for Content Retrieval",id:"thread-pool-for-content-retrieval",level:3},{value:"Producer-Consumer Pipeline",id:"producer-consumer-pipeline",level:3},{value:"Memory Management and Caching",id:"memory-management-and-caching",level:2},{value:"Content Caching",id:"content-caching",level:3},{value:"Memory-Mapped Content Access",id:"memory-mapped-content-access",level:3},{value:"Monitoring and Performance Tuning",id:"monitoring-and-performance-tuning",level:2},{value:"Performance Metrics Collection",id:"performance-metrics-collection",level:3},{value:"Dynamic Parameter Tuning",id:"dynamic-parameter-tuning",level:3},{value:"End-to-End Pipeline Implementation",id:"end-to-end-pipeline-implementation",level:2},{value:"Complete Rust Implementation",id:"complete-rust-implementation",level:3},{value:"Complete Python Implementation",id:"complete-python-implementation",level:3},{value:"Conclusion",id:"conclusion",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"maximizing-indexing-throughput-with-content-storage-integration",children:"Maximizing Indexing Throughput with Content Storage Integration"})}),"\n",(0,s.jsx)(n.p,{children:"This guide provides comprehensive strategies and practical examples for achieving maximum indexing throughput with Fast Inverted Index while efficiently integrating with external content storage systems. It covers optimizations for index creation, content storage integration, and system-level performance tuning."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Fast Inverted Index is designed to store the inverted index (terms to document mappings) separate from document content. This architectural separation offers flexibility and specialization but requires careful integration for optimal performance. This guide will help you achieve high-performance indexing while maintaining this separation of concerns."}),"\n",(0,s.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#throughput-optimization-principles",children:"Throughput Optimization Principles"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#rocksdb-optimization-for-maximum-indexing-speed",children:"RocksDB Optimization for Maximum Indexing Speed"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#content-storage-selection-and-integration",children:"Content Storage Selection and Integration"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#batched-processing-patterns",children:"Batched Processing Patterns"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#parallel-indexing-strategies",children:"Parallel Indexing Strategies"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#memory-management-and-caching",children:"Memory Management and Caching"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#monitoring-and-performance-tuning",children:"Monitoring and Performance Tuning"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#end-to-end-pipeline-implementation",children:"End-to-End Pipeline Implementation"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"throughput-optimization-principles",children:"Throughput Optimization Principles"}),"\n",(0,s.jsx)(n.h3,{id:"key-performance-factors",children:"Key Performance Factors"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Size"})," - Larger batches enable economies of scale in processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallelism"})," - Utilizing multiple cores efficiently"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"I/O Patterns"})," - Optimizing read/write access to both content and index"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Utilization"})," - Appropriate buffering and caching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Serialization Efficiency"})," - Minimizing conversion overhead"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"metrics-to-monitor",children:"Metrics to Monitor"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Get performance metrics\nlet stats = index.metrics.summary();\n\nprintln!("Documents: {}", stats.document_count);\nprintln!("Indexing Rate: {} docs/sec", stats.indexing_rate);\nprintln!("Batch Write Rate: {} ops/sec", stats.batch_write_rate);\nprintln!("Avg Batch Size: {}", stats.avg_batch_size);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Get performance metrics\nstats = index.stats()\n\nprint(f\"Documents: {stats['document_count']}\")\nprint(f\"Indexing Rate: {stats['indexing_rate']} docs/sec\")\nprint(f\"Batch Write Rate: {stats['batch_write_rate']} ops/sec\")\nprint(f\"Avg Batch Size: {stats['avg_batch_size']}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"rocksdb-optimization-for-maximum-indexing-speed",children:"RocksDB Optimization for Maximum Indexing Speed"}),"\n",(0,s.jsx)(n.p,{children:"The storage layer configuration is critical for high-throughput indexing. Our optimized RocksDB implementation provides several features specifically designed for bulk loading."}),"\n",(0,s.jsx)(n.h3,{id:"bulk-loading-configuration",children:"Bulk Loading Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Optimized RocksDB configuration for bulk loading\nuse fast_inverted_index::storage::RocksDbConfig;\n\nlet config = RocksDbConfig {\n    path: "/path/to/index".into(),\n    \n    // Large write buffers for batch efficiency\n    write_buffer_size: 256 * 1024 * 1024, // 256 MB \n    max_write_buffer_number: 6,\n    \n    // Bulk loading optimizations\n    disable_wal_for_bulk: true,  // Critical for performance\n    use_write_batch: true,\n    write_batch_size: 10000,     // Large batch size\n    \n    // Parallelism settings\n    max_background_jobs: num_cpus::get() as i32,\n    \n    // Enable optimizations for initial load\n    optimize_for_bulk: true,\n    \n    // Other settings with defaults\n    ..Default::default()\n};\n\n// Create index with this configuration\nlet index = IndexBuilder::new()\n    .with_storage_config(config)\n    .build()?;\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Configure RocksDB through environment variables\n# (since Python API doesn\'t directly expose all RocksDB settings)\nimport os\n\n# Set environment variables for RocksDB optimization\nos.environ["ROCKSDB_WRITE_BUFFER_SIZE"] = str(256 * 1024 * 1024)  # 256 MB\nos.environ["ROCKSDB_MAX_WRITE_BUFFER_NUMBER"] = "6"\nos.environ["ROCKSDB_DISABLE_WAL"] = "1"  # Disable WAL for bulk loading\nos.environ["ROCKSDB_MAX_BACKGROUND_JOBS"] = str(os.cpu_count())\nos.environ["ROCKSDB_OPTIMIZE_FOR_BULK"] = "1"\n\n# Create index with storage path\nfrom fast_inverted_index import Index, Schema\n\nschema = Schema()\n# Configure schema...\n\nindex = Index(\n    storage_path="/path/to/index",\n    in_memory=False,  # Use RocksDB storage\n    schema=schema\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"enabling-bulk-loading-mode",children:"Enabling Bulk Loading Mode"}),"\n",(0,s.jsx)(n.p,{children:"For existing indexes, you can enable bulk loading mode before large operations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Enable bulk loading mode for an existing store\nif let Some(storage) = index.storage_as_rocksdb_mut() {\n    // Enable bulk loading mode (significantly improves write performance)\n    storage.set_bulk_loading_mode(true);\n    \n    // ... perform bulk indexing operations ...\n    \n    // Disable bulk loading mode when finished (for durability)\n    storage.set_bulk_loading_mode(false);\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:"In Python, this would be done via the environment variables before creating the index."}),"\n",(0,s.jsx)(n.h3,{id:"writebatch-optimization",children:"WriteBatch Optimization"}),"\n",(0,s.jsx)(n.p,{children:"The WriteBatch feature consolidates multiple write operations into a single atomic batch:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Manual batch operations (lower level API)\n// (the index automatically uses batching internally)\nlet mut batch = WriteBatch::default();\n\n// Add multiple operations to the batch\nfor (i, doc) in documents.iter().enumerate() {\n    batch.put_cf(cf_docs, &i.to_be_bytes(), &serialize(doc)?);\n}\n\n// Execute the batch in one operation\ndb.write(batch)?;\n"})}),"\n",(0,s.jsx)(n.p,{children:"In Python, batching happens automatically when you use the batch document addition methods."}),"\n",(0,s.jsx)(n.h2,{id:"content-storage-selection-and-integration",children:"Content Storage Selection and Integration"}),"\n",(0,s.jsx)(n.h3,{id:"selecting-the-right-content-storage",children:"Selecting the Right Content Storage"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Storage Type"}),(0,s.jsx)(n.th,{children:"Best For"}),(0,s.jsx)(n.th,{children:"Integration Complexity"}),(0,s.jsx)(n.th,{children:"Performance Characteristics"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Filesystem"}),(0,s.jsx)(n.td,{children:"Simple deployments, static content"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Fast writes, medium reads"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RDBMS (PostgreSQL/MySQL)"}),(0,s.jsx)(n.td,{children:"Structured data, complex queries"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Medium writes, medium reads"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Document DB (MongoDB)"}),(0,s.jsx)(n.td,{children:"Semi-structured data, simple scaling"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Fast writes, fast reads"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Key-Value (Redis)"}),(0,s.jsx)(n.td,{children:"High-throughput, cached content"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Very fast writes, very fast reads"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Object Storage (S3)"}),(0,s.jsx)(n.td,{children:"Large-scale, cost-effective"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Slow writes, medium reads"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Distributed (Cassandra)"}),(0,s.jsx)(n.td,{children:"Massive scale, high availability"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Fast writes, medium reads"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"content-store-integration-patterns",children:"Content Store Integration Patterns"}),"\n",(0,s.jsx)(n.h4,{id:"direct-integration",children:"Direct Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Direct content store integration\nstruct IntegratedSearch {\n    index: Index,\n    content_store: ContentStore,\n}\n\nimpl IntegratedSearch {\n    // Add a document with content\n    pub fn add_document(&mut self, doc_id: DocId, content: &str, metadata: &Metadata) -> Result<()> {\n        // 1. Store content first\n        self.content_store.store_document(doc_id, content, metadata)?;\n        \n        // 2. Index the document (content is tokenized but not stored)\n        self.index.add_document(doc_id, content, metadata)?;\n        \n        Ok(())\n    }\n    \n    // Search and retrieve content\n    pub fn search(&self, query: &str) -> Result<Vec<(DocId, f32, String)>> {\n        // 1. Search the index\n        let results = self.index.search(query)?;\n        \n        // 2. Retrieve content for matches\n        let mut full_results = Vec::with_capacity(results.len());\n        for (doc_id, score) in results {\n            if let Some(content) = self.content_store.get_document(doc_id)? {\n                full_results.push((doc_id, score, content));\n            }\n        }\n        \n        Ok(full_results)\n    }\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Direct content store integration\nimport redis\nfrom fast_inverted_index import Index, Schema\n\nclass IntegratedSearch:\n    def __init__(self, index_path, redis_host=\'localhost\'):\n        # Set up index\n        self.schema = Schema()\n        # Configure schema...\n        self.index = Index(storage_path=index_path, schema=self.schema)\n        \n        # Set up content store (Redis in this example)\n        self.content_store = redis.Redis(host=redis_host)\n        \n    def add_document(self, doc_id, content, metadata=None):\n        # 1. Store content in Redis\n        self.content_store.set(f"doc:{doc_id}", content)\n        \n        # 2. Store metadata separately if needed\n        if metadata:\n            self.content_store.hset(f"meta:{doc_id}", mapping=metadata)\n        \n        # 3. Index the document\n        self.index.add_document(doc_id, content, metadata)\n        \n    def search(self, query, limit=10):\n        # 1. Search the index\n        results = self.index.search(query, limit=limit)\n        \n        # 2. Retrieve content for matches\n        full_results = []\n        for doc_id, score in results:\n            content = self.content_store.get(f"doc:{doc_id}")\n            if content:\n                full_results.append((doc_id, score, content.decode(\'utf-8\')))\n                \n        return full_results\n'})}),"\n",(0,s.jsx)(n.h4,{id:"pipeline-integration",children:"Pipeline Integration"}),"\n",(0,s.jsx)(n.p,{children:"For more complex scenarios, use a processing pipeline:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Pipeline pattern with queuing\nuse std::sync::mpsc::{channel, Sender, Receiver};\nuse std::thread;\n\nstruct IndexingPipeline {\n    content_store: ContentStore,\n    index: Index,\n    sender: Sender<(DocId, String, Metadata)>,\n}\n\nimpl IndexingPipeline {\n    pub fn new(content_store: ContentStore, index: Index) -> Self {\n        let (sender, receiver) = channel();\n        \n        // Start indexing worker thread\n        let index_worker = index.clone();\n        thread::spawn(move || {\n            // Process documents from the queue\n            for (doc_id, content, metadata) in receiver {\n                if let Err(e) = index_worker.add_document(doc_id, &content, &metadata) {\n                    eprintln!("Error indexing document {}: {}", doc_id, e);\n                }\n            }\n        });\n        \n        Self { content_store, index, sender }\n    }\n    \n    pub fn add_document(&mut self, doc_id: DocId, content: &str, metadata: &Metadata) -> Result<()> {\n        // 1. Store content immediately\n        self.content_store.store_document(doc_id, content, metadata)?;\n        \n        // 2. Queue for asynchronous indexing\n        self.sender.send((doc_id, content.to_string(), metadata.clone()))?;\n        \n        Ok(())\n    }\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Pipeline pattern with queuing\nimport threading\nimport queue\n\nclass IndexingPipeline:\n    def __init__(self, content_store, index, queue_size=10000):\n        self.content_store = content_store\n        self.index = index\n        self.queue = queue.Queue(maxsize=queue_size)\n        \n        # Start worker thread\n        self.worker = threading.Thread(target=self._indexing_worker, daemon=True)\n        self.worker.start()\n        \n    def _indexing_worker(self):\n        while True:\n            try:\n                # Get document from queue\n                doc_id, content, metadata = self.queue.get()\n                \n                # Index the document\n                self.index.add_document(doc_id, content, metadata)\n                \n                # Mark task as done\n                self.queue.task_done()\n            except Exception as e:\n                print(f"Error in indexing worker: {e}")\n    \n    def add_document(self, doc_id, content, metadata=None):\n        # Store content immediately\n        self.content_store.store_document(doc_id, content, metadata)\n        \n        # Queue for asynchronous indexing\n        self.queue.put((doc_id, content, metadata))\n        \n    def wait_completion(self):\n        # Wait for all queued documents to be indexed\n        self.queue.join()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"batched-processing-patterns",children:"Batched Processing Patterns"}),"\n",(0,s.jsx)(n.p,{children:"Batching is critical for maximizing throughput. Here are patterns for both Rust and Python:"}),"\n",(0,s.jsx)(n.h3,{id:"batch-document-addition",children:"Batch Document Addition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Batch document addition\nuse std::collections::HashMap;\n\n// Prepare a batch of documents\nlet mut documents = Vec::with_capacity(10000);\nlet mut doc_id = 0;\n\n// Generate or load 10,000 documents\nfor _ in 0..10000 {\n    let content = format!("Document content for doc {}", doc_id);\n    let metadata = HashMap::from([\n        ("title".to_string(), format!("Document {}", doc_id)),\n        ("category".to_string(), "batch example".to_string()),\n    ]);\n    \n    documents.push((doc_id, content, metadata));\n    doc_id += 1;\n}\n\n// Add documents in a single batch operation\nindex.add_documents_batch(documents)?;\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Batch document addition\n# Prepare a batch of documents\ndocuments = []\nfor i in range(10000):\n    doc_id = i\n    content = f"Document content for doc {doc_id}"\n    metadata = {\n        "title": f"Document {doc_id}",\n        "category": "batch example"\n    }\n    documents.append((doc_id, {"content": content, **metadata}))\n\n# Add documents in a single batch operation\nindex.add_documents_with_fields_parallel(documents)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"optimized-batch-content-retrieval",children:"Optimized Batch Content Retrieval"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Optimized batch content retrieval\npub fn batch_retrieve_and_index<S: ContentStore>(\n    content_store: &S,\n    index: &mut Index,\n    doc_ids: &[DocId]\n) -> Result<()> {\n    // 1. Retrieve documents in batch\n    let documents = content_store.batch_get_documents(doc_ids)?;\n    \n    // 2. Prepare for indexing\n    let mut batch = Vec::with_capacity(documents.len());\n    for (doc_id, content, metadata) in documents {\n        batch.push((doc_id, content, metadata));\n    }\n    \n    // 3. Index in a single batch operation\n    index.add_documents_batch(batch)?;\n    \n    Ok(())\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Optimized batch content retrieval\ndef batch_retrieve_and_index(content_store, index, doc_ids):\n    # 1. Retrieve documents in batch\n    documents = content_store.batch_get_documents(doc_ids)\n    \n    # 2. Prepare for indexing\n    batch = []\n    for doc_id, content, metadata in documents:\n        batch.append((doc_id, {"content": content, **metadata}))\n    \n    # 3. Index in a single batch operation\n    index.add_documents_with_fields_parallel(batch)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multi-stage-batching",children:"Multi-stage Batching"}),"\n",(0,s.jsx)(n.p,{children:"For very large document collections, use multi-stage batching:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Multi-stage batching\nconst BATCH_SIZE: usize = 10000;\n\npub fn index_large_collection<I, S>(\n    index: &mut Index,\n    documents: I\n) -> Result<()>\nwhere\n    I: Iterator<Item = (DocId, String, HashMap<String, String>)>,\n{\n    let mut batch = Vec::with_capacity(BATCH_SIZE);\n    let mut total_indexed = 0;\n    \n    // Enable bulk loading mode if using RocksDB\n    if let Some(storage) = index.storage_as_rocksdb_mut() {\n        storage.set_bulk_loading_mode(true);\n    }\n    \n    // Process documents in batches\n    for document in documents {\n        batch.push(document);\n        \n        // When batch is full, index and create a new batch\n        if batch.len() >= BATCH_SIZE {\n            index.add_documents_batch(&batch)?;\n            total_indexed += batch.len();\n            println!("Indexed {} documents so far", total_indexed);\n            batch.clear();\n        }\n    }\n    \n    // Index remaining documents\n    if !batch.is_empty() {\n        index.add_documents_batch(&batch)?;\n        total_indexed += batch.len();\n    }\n    \n    // Disable bulk loading mode and optimize\n    if let Some(storage) = index.storage_as_rocksdb_mut() {\n        storage.set_bulk_loading_mode(false);\n    }\n    \n    println!("Completed indexing {} documents", total_indexed);\n    Ok(())\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Multi-stage batching\nBATCH_SIZE = 10000\n\ndef index_large_collection(index, documents_iterator):\n    batch = []\n    total_indexed = 0\n    \n    # Process documents in batches\n    for doc_id, content, metadata in documents_iterator:\n        batch.append((doc_id, {"content": content, **metadata}))\n        \n        # When batch is full, index and create a new batch\n        if len(batch) >= BATCH_SIZE:\n            index.add_documents_with_fields_parallel(batch)\n            total_indexed += len(batch)\n            print(f"Indexed {total_indexed} documents so far")\n            batch = []\n    \n    # Index remaining documents\n    if batch:\n        index.add_documents_with_fields_parallel(batch)\n        total_indexed += len(batch)\n    \n    print(f"Completed indexing {total_indexed} documents")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"parallel-indexing-strategies",children:"Parallel Indexing Strategies"}),"\n",(0,s.jsx)(n.h3,{id:"thread-pool-for-content-retrieval",children:"Thread Pool for Content Retrieval"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Thread pool for content retrieval\nuse rayon::prelude::*;\n\npub fn parallel_retrieve_and_index<S: ContentStore + Sync>(\n    content_store: &S,\n    index: &mut Index,\n    doc_ids: &[DocId],\n    num_threads: usize\n) -> Result<()> {\n    // Split doc_ids into chunks\n    let chunk_size = (doc_ids.len() + num_threads - 1) / num_threads;\n    let chunks: Vec<_> = doc_ids.chunks(chunk_size).collect();\n    \n    // Process chunks in parallel\n    let results: Result<Vec<Vec<_>>> = chunks.par_iter()\n        .map(|chunk| {\n            // Retrieve documents for this chunk\n            content_store.batch_get_documents(chunk)\n        })\n        .collect();\n    \n    // Flatten results\n    let documents = results?.into_iter().flatten().collect::<Vec<_>>();\n    \n    // Index all documents in a single batch\n    index.add_documents_batch(documents)?;\n    \n    Ok(())\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Thread pool for content retrieval\nimport concurrent.futures\n\ndef parallel_retrieve_and_index(content_store, index, doc_ids, num_threads=8):\n    # Split doc_ids into chunks\n    chunk_size = (len(doc_ids) + num_threads - 1) // num_threads\n    chunks = [doc_ids[i:i+chunk_size] for i in range(0, len(doc_ids), chunk_size)]\n    \n    # Function to process a chunk\n    def process_chunk(chunk):\n        return content_store.batch_get_documents(chunk)\n    \n    # Process chunks in parallel\n    documents = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n        future_to_chunk = {executor.submit(process_chunk, chunk): chunk for chunk in chunks}\n        for future in concurrent.futures.as_completed(future_to_chunk):\n            documents.extend(future.result())\n    \n    # Prepare for indexing\n    batch = [(doc_id, {"content": content, **metadata}) for doc_id, content, metadata in documents]\n    \n    # Index in a single batch operation\n    index.add_documents_with_fields_parallel(batch)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"producer-consumer-pipeline",children:"Producer-Consumer Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Producer-consumer pipeline\nuse std::sync::mpsc::{channel, Sender, Receiver};\nuse std::thread;\nuse std::sync::{Arc, Mutex};\n\npub struct IndexingPipeline {\n    content_queue: Sender<DocId>,\n    document_queue: Arc<Mutex<Vec<(DocId, String, HashMap<String, String>)>>>,\n    max_batch_size: usize,\n    content_store: Arc<dyn ContentStore + Send + Sync>,\n    index: Arc<Mutex<Index>>,\n}\n\nimpl IndexingPipeline {\n    pub fn new(\n        content_store: Arc<dyn ContentStore + Send + Sync>,\n        index: Index,\n        num_threads: usize,\n        max_batch_size: usize\n    ) -> Self {\n        let (content_sender, content_receiver) = channel();\n        let document_queue = Arc::new(Mutex::new(Vec::with_capacity(max_batch_size)));\n        let index = Arc::new(Mutex::new(index));\n        \n        // Start content retrieval workers\n        for _ in 0..num_threads {\n            let content_store = Arc::clone(&content_store);\n            let document_queue = Arc::clone(&document_queue);\n            let content_receiver = content_receiver.clone();\n            let index = Arc::clone(&index);\n            let max_batch_size = max_batch_size;\n            \n            thread::spawn(move || {\n                for doc_id in content_receiver {\n                    // Retrieve document\n                    if let Ok((content, metadata)) = content_store.get_document(doc_id) {\n                        // Add to document queue\n                        {\n                            let mut queue = document_queue.lock().unwrap();\n                            queue.push((doc_id, content, metadata));\n                            \n                            // When batch is full, index and clear\n                            if queue.len() >= max_batch_size {\n                                let batch = std::mem::replace(\n                                    &mut *queue, \n                                    Vec::with_capacity(max_batch_size)\n                                );\n                                \n                                // Release lock before indexing\n                                drop(queue);\n                                \n                                // Index the batch\n                                if let Ok(mut index_guard) = index.lock() {\n                                    let _ = index_guard.add_documents_batch(&batch);\n                                }\n                            }\n                        }\n                    }\n                }\n            });\n        }\n        \n        Self {\n            content_queue: content_sender,\n            document_queue,\n            max_batch_size,\n            content_store,\n            index,\n        }\n    }\n    \n    pub fn queue_document(&self, doc_id: DocId) -> Result<()> {\n        self.content_queue.send(doc_id)?;\n        Ok(())\n    }\n    \n    pub fn flush(&self) -> Result<()> {\n        // Index any remaining documents\n        let batch = {\n            let mut queue = self.document_queue.lock().unwrap();\n            std::mem::replace(&mut *queue, Vec::with_capacity(self.max_batch_size))\n        };\n        \n        if !batch.is_empty() {\n            let mut index = self.index.lock().unwrap();\n            index.add_documents_batch(&batch)?;\n        }\n        \n        Ok(())\n    }\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Producer-consumer pipeline\nimport threading\nimport queue\n\nclass IndexingPipeline:\n    def __init__(self, content_store, index, num_threads=8, max_batch_size=10000):\n        self.content_store = content_store\n        self.index = index\n        self.max_batch_size = max_batch_size\n        \n        # Queues\n        self.id_queue = queue.Queue()\n        self.doc_queue = queue.Queue()\n        \n        # Control flags\n        self.shutdown = threading.Event()\n        \n        # Start content retrieval workers\n        self.content_workers = []\n        for _ in range(num_threads):\n            worker = threading.Thread(\n                target=self._content_worker,\n                daemon=True\n            )\n            worker.start()\n            self.content_workers.append(worker)\n        \n        # Start indexing worker\n        self.indexing_worker = threading.Thread(\n            target=self._indexing_worker,\n            daemon=True\n        )\n        self.indexing_worker.start()\n    \n    def _content_worker(self):\n        while not self.shutdown.is_set():\n            try:\n                # Get document ID from queue with timeout\n                doc_id = self.id_queue.get(timeout=0.1)\n                \n                # Retrieve document\n                content, metadata = self.content_store.get_document(doc_id)\n                \n                # Add to document queue\n                self.doc_queue.put((doc_id, content, metadata))\n                \n                # Mark task as done\n                self.id_queue.task_done()\n            except queue.Empty:\n                continue\n            except Exception as e:\n                print(f"Error in content worker: {e}")\n    \n    def _indexing_worker(self):\n        batch = []\n        \n        while not self.shutdown.is_set():\n            try:\n                # Get document from queue with timeout\n                doc_id, content, metadata = self.doc_queue.get(timeout=0.1)\n                \n                # Add to batch\n                batch.append((doc_id, {"content": content, **metadata}))\n                \n                # Mark task as done\n                self.doc_queue.task_done()\n                \n                # When batch is full or queue is empty, index and create new batch\n                if len(batch) >= self.max_batch_size or self.doc_queue.empty():\n                    if batch:\n                        self.index.add_documents_with_fields_parallel(batch)\n                        batch = []\n            except queue.Empty:\n                # Index any documents in the batch\n                if batch:\n                    self.index.add_documents_with_fields_parallel(batch)\n                    batch = []\n            except Exception as e:\n                print(f"Error in indexing worker: {e}")\n    \n    def queue_document(self, doc_id):\n        self.id_queue.put(doc_id)\n    \n    def flush(self):\n        # Wait for all queued documents to be processed\n        self.id_queue.join()\n        self.doc_queue.join()\n    \n    def shutdown(self):\n        self.shutdown.set()\n        self.flush()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"memory-management-and-caching",children:"Memory Management and Caching"}),"\n",(0,s.jsx)(n.h3,{id:"content-caching",children:"Content Caching"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Content caching layer\nuse lru::LruCache;\nuse std::sync::Mutex;\nuse std::num::NonZeroUsize;\n\npub struct CachedContentStore<S: ContentStore> {\n    store: S,\n    cache: Mutex<LruCache<DocId, (String, HashMap<String, String>)>>,\n}\n\nimpl<S: ContentStore> CachedContentStore<S> {\n    pub fn new(store: S, capacity: usize) -> Self {\n        Self {\n            store,\n            cache: Mutex::new(LruCache::new(\n                NonZeroUsize::new(capacity).unwrap())\n            ),\n        }\n    }\n}\n\nimpl<S: ContentStore> ContentStore for CachedContentStore<S> {\n    fn get_document(&self, doc_id: DocId) \n        -> Result<(String, HashMap<String, String>)> \n    {\n        // Check cache first\n        {\n            let mut cache = self.cache.lock().unwrap();\n            if let Some(doc) = cache.get(&doc_id) {\n                return Ok(doc.clone());\n            }\n        }\n        \n        // Not in cache, retrieve from store\n        let document = self.store.get_document(doc_id)?;\n        \n        // Add to cache\n        {\n            let mut cache = self.cache.lock().unwrap();\n            cache.put(doc_id, document.clone());\n        }\n        \n        Ok(document)\n    }\n    \n    fn batch_get_documents(&self, doc_ids: &[DocId]) \n        -> Result<Vec<(DocId, String, HashMap<String, String>)>> \n    {\n        let mut result = Vec::with_capacity(doc_ids.len());\n        let mut missing_ids = Vec::new();\n        \n        // Get cached documents\n        {\n            let mut cache = self.cache.lock().unwrap();\n            for &doc_id in doc_ids {\n                if let Some(doc) = cache.get(&doc_id) {\n                    result.push((doc_id, doc.0.clone(), doc.1.clone()));\n                } else {\n                    missing_ids.push(doc_id);\n                }\n            }\n        }\n        \n        // Get missing documents from store\n        if !missing_ids.is_empty() {\n            let missing_docs = self.store.batch_get_documents(&missing_ids)?;\n            \n            // Add to cache and result\n            let mut cache = self.cache.lock().unwrap();\n            for (doc_id, content, metadata) in missing_docs {\n                cache.put(doc_id, (content.clone(), metadata.clone()));\n                result.push((doc_id, content, metadata));\n            }\n        }\n        \n        Ok(result)\n    }\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Content caching layer\nimport functools\nfrom cachetools import LRUCache\nimport threading\n\nclass CachedContentStore:\n    def __init__(self, store, capacity=10000):\n        self.store = store\n        self.cache = LRUCache(maxsize=capacity)\n        self.lock = threading.RLock()\n    \n    def get_document(self, doc_id):\n        # Check cache first\n        with self.lock:\n            if doc_id in self.cache:\n                return self.cache[doc_id]\n        \n        # Not in cache, retrieve from store\n        document = self.store.get_document(doc_id)\n        \n        # Add to cache\n        with self.lock:\n            self.cache[doc_id] = document\n        \n        return document\n    \n    def batch_get_documents(self, doc_ids):\n        result = []\n        missing_ids = []\n        \n        # Get cached documents\n        with self.lock:\n            for doc_id in doc_ids:\n                if doc_id in self.cache:\n                    content, metadata = self.cache[doc_id]\n                    result.append((doc_id, content, metadata))\n                else:\n                    missing_ids.append(doc_id)\n        \n        # Get missing documents from store\n        if missing_ids:\n            missing_docs = self.store.batch_get_documents(missing_ids)\n            \n            # Add to cache and result\n            with self.lock:\n                for doc_id, content, metadata in missing_docs:\n                    self.cache[doc_id] = (content, metadata)\n                    result.append((doc_id, content, metadata))\n        \n        return result\n"})}),"\n",(0,s.jsx)(n.h3,{id:"memory-mapped-content-access",children:"Memory-Mapped Content Access"}),"\n",(0,s.jsx)(n.p,{children:"For larger-than-memory content collections:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Memory-mapped content access\nuse memmap2::{Mmap, MmapOptions};\nuse std::fs::File;\nuse std::io::{BufReader, BufWriter, Read, Seek, SeekFrom, Write};\nuse std::path::{Path, PathBuf};\n\npub struct MmapContentStore {\n    content_file: PathBuf,\n    index_file: PathBuf,\n    mmap: Mmap,\n    offsets: HashMap<DocId, (usize, usize)>, // (offset, length)\n}\n\nimpl MmapContentStore {\n    pub fn new<P: AsRef<Path>>(content_file: P, index_file: P) -> Result<Self> {\n        // Open the content file\n        let file = File::open(&content_file)?;\n        let mmap = unsafe { MmapOptions::new().map(&file)? };\n        \n        // Load the offsets from the index file\n        let index_file = File::open(&index_file)?;\n        let reader = BufReader::new(index_file);\n        let offsets: HashMap<DocId, (usize, usize)> = bincode::deserialize_from(reader)?;\n        \n        Ok(Self {\n            content_file: content_file.as_ref().to_path_buf(),\n            index_file: index_file.as_ref().to_path_buf(),\n            mmap,\n            offsets,\n        })\n    }\n}\n\nimpl ContentStore for MmapContentStore {\n    fn get_document(&self, doc_id: DocId) \n        -> Result<(String, HashMap<String, String>)> \n    {\n        // Get the document offset and length\n        let (offset, length) = self.offsets.get(&doc_id)\n            .ok_or_else(|| Error::DocumentNotFound(doc_id))?;\n        \n        // Read the document content\n        let content_bytes = &self.mmap[*offset..*offset + *length];\n        let (content, metadata): (String, HashMap<String, String>) = \n            bincode::deserialize(content_bytes)?;\n        \n        Ok((content, metadata))\n    }\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Memory-mapped content access (using mmap)\nimport mmap\nimport os\nimport json\nimport struct\n\nclass MmapContentStore:\n    def __init__(self, content_file, index_file):\n        # Open the content file\n        self.content_fd = open(content_file, 'rb')\n        self.mmap = mmap.mmap(self.content_fd.fileno(), 0, access=mmap.ACCESS_READ)\n        \n        # Load the offsets from the index file\n        with open(index_file, 'rb') as f:\n            index_data = f.read()\n            \n        # Parse the index (format: doc_id(8 bytes), offset(8 bytes), length(8 bytes))\n        self.offsets = {}\n        for i in range(0, len(index_data), 24):\n            doc_id = struct.unpack('<Q', index_data[i:i+8])[0]\n            offset = struct.unpack('<Q', index_data[i+8:i+16])[0]\n            length = struct.unpack('<Q', index_data[i+16:i+24])[0]\n            self.offsets[doc_id] = (offset, length)\n    \n    def get_document(self, doc_id):\n        # Get the document offset and length\n        if doc_id not in self.offsets:\n            raise KeyError(f\"Document {doc_id} not found\")\n            \n        offset, length = self.offsets[doc_id]\n        \n        # Read the document content\n        self.mmap.seek(offset)\n        data = self.mmap.read(length)\n        \n        # Parse the document (format: content length(4 bytes), content, metadata as JSON)\n        content_length = struct.unpack('<I', data[:4])[0]\n        content = data[4:4+content_length].decode('utf-8')\n        metadata_json = data[4+content_length:].decode('utf-8')\n        metadata = json.loads(metadata_json)\n        \n        return content, metadata\n    \n    def close(self):\n        self.mmap.close()\n        self.content_fd.close()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-and-performance-tuning",children:"Monitoring and Performance Tuning"}),"\n",(0,s.jsx)(n.h3,{id:"performance-metrics-collection",children:"Performance Metrics Collection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Performance metrics collection\nuse std::time::{Duration, Instant};\nuse std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};\n\npub struct IndexingMetrics {\n    start_time: Instant,\n    document_count: AtomicUsize,\n    token_count: AtomicUsize,\n    processing_time_ns: AtomicU64,\n    indexing_time_ns: AtomicU64,\n    batch_count: AtomicUsize,\n}\n\nimpl IndexingMetrics {\n    pub fn new() -> Self {\n        Self {\n            start_time: Instant::now(),\n            document_count: AtomicUsize::new(0),\n            token_count: AtomicUsize::new(0),\n            processing_time_ns: AtomicU64::new(0),\n            indexing_time_ns: AtomicU64::new(0),\n            batch_count: AtomicUsize::new(0),\n        }\n    }\n    \n    pub fn record_batch(&self, doc_count: usize, token_count: usize,\n                         processing_time: Duration, indexing_time: Duration) {\n        self.document_count.fetch_add(doc_count, Ordering::Relaxed);\n        self.token_count.fetch_add(token_count, Ordering::Relaxed);\n        self.processing_time_ns.fetch_add(processing_time.as_nanos() as u64, \n                                          Ordering::Relaxed);\n        self.indexing_time_ns.fetch_add(indexing_time.as_nanos() as u64, \n                                        Ordering::Relaxed);\n        self.batch_count.fetch_add(1, Ordering::Relaxed);\n    }\n    \n    pub fn summary(&self) -> String {\n        let elapsed = self.start_time.elapsed();\n        let doc_count = self.document_count.load(Ordering::Relaxed);\n        let token_count = self.token_count.load(Ordering::Relaxed);\n        let processing_time_ns = self.processing_time_ns.load(Ordering::Relaxed);\n        let indexing_time_ns = self.indexing_time_ns.load(Ordering::Relaxed);\n        let batch_count = self.batch_count.load(Ordering::Relaxed);\n        \n        // Calculate rates\n        let docs_per_sec = if elapsed.as_secs() > 0 {\n            doc_count as f64 / elapsed.as_secs_f64()\n        } else {\n            0.0\n        };\n        \n        let tokens_per_sec = if elapsed.as_secs() > 0 {\n            token_count as f64 / elapsed.as_secs_f64()\n        } else {\n            0.0\n        };\n        \n        let avg_batch_size = if batch_count > 0 {\n            doc_count as f64 / batch_count as f64\n        } else {\n            0.0\n        };\n        \n        format!(\n            "Indexing Metrics:\\n\\\n             - Documents: {}\\n\\\n             - Tokens: {}\\n\\\n             - Elapsed Time: {:.2?}\\n\\\n             - Docs/sec: {:.2}\\n\\\n             - Tokens/sec: {:.2}\\n\\\n             - Avg Batch Size: {:.2}\\n\\\n             - Processing Time: {:.2?}\\n\\\n             - Indexing Time: {:.2?}",\n            doc_count,\n            token_count,\n            elapsed,\n            docs_per_sec,\n            tokens_per_sec,\n            avg_batch_size,\n            Duration::from_nanos(processing_time_ns),\n            Duration::from_nanos(indexing_time_ns)\n        )\n    }\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Performance metrics collection\nimport time\nimport threading\nfrom collections import defaultdict\n\nclass IndexingMetrics:\n    def __init__(self):\n        self.start_time = time.time()\n        self.document_count = 0\n        self.token_count = 0\n        self.processing_time = 0.0\n        self.indexing_time = 0.0\n        self.batch_count = 0\n        self.lock = threading.RLock()\n    \n    def record_batch(self, doc_count, token_count, processing_time, indexing_time):\n        with self.lock:\n            self.document_count += doc_count\n            self.token_count += token_count\n            self.processing_time += processing_time\n            self.indexing_time += indexing_time\n            self.batch_count += 1\n    \n    def summary(self):\n        with self.lock:\n            elapsed = time.time() - self.start_time\n            \n            # Calculate rates\n            docs_per_sec = self.document_count / elapsed if elapsed > 0 else 0\n            tokens_per_sec = self.token_count / elapsed if elapsed > 0 else 0\n            avg_batch_size = self.document_count / self.batch_count if self.batch_count > 0 else 0\n            \n            return {\n                "documents": self.document_count,\n                "tokens": self.token_count,\n                "elapsed_seconds": elapsed,\n                "docs_per_sec": docs_per_sec,\n                "tokens_per_sec": tokens_per_sec,\n                "avg_batch_size": avg_batch_size,\n                "processing_time": self.processing_time,\n                "indexing_time": self.indexing_time\n            }\n    \n    def print_summary(self):\n        summary = self.summary()\n        print(f"Indexing Metrics:")\n        print(f"- Documents: {summary[\'documents\']}")\n        print(f"- Tokens: {summary[\'tokens\']}")\n        print(f"- Elapsed Time: {summary[\'elapsed_seconds\']:.2f}s")\n        print(f"- Docs/sec: {summary[\'docs_per_sec\']:.2f}")\n        print(f"- Tokens/sec: {summary[\'tokens_per_sec\']:.2f}")\n        print(f"- Avg Batch Size: {summary[\'avg_batch_size\']:.2f}")\n        print(f"- Processing Time: {summary[\'processing_time\']:.2f}s")\n        print(f"- Indexing Time: {summary[\'indexing_time\']:.2f}s")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"dynamic-parameter-tuning",children:"Dynamic Parameter Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Dynamic parameter tuning\npub struct DynamicTuner {\n    metrics: Arc<IndexingMetrics>,\n    last_adjustment: Instant,\n    adjustment_interval: Duration,\n    min_threads: usize,\n    max_threads: usize,\n    min_batch_size: usize,\n    max_batch_size: usize,\n    current_threads: AtomicUsize,\n    current_batch_size: AtomicUsize,\n}\n\nimpl DynamicTuner {\n    pub fn new(\n        metrics: Arc<IndexingMetrics>,\n        min_threads: usize,\n        max_threads: usize,\n        min_batch_size: usize,\n        max_batch_size: usize\n    ) -> Self {\n        let initial_threads = std::cmp::min(num_cpus::get(), max_threads);\n        let initial_batch_size = 1000;\n        \n        Self {\n            metrics,\n            last_adjustment: Instant::now(),\n            adjustment_interval: Duration::from_secs(30),\n            min_threads,\n            max_threads,\n            min_batch_size,\n            max_batch_size,\n            current_threads: AtomicUsize::new(initial_threads),\n            current_batch_size: AtomicUsize::new(initial_batch_size),\n        }\n    }\n    \n    pub fn check_and_adjust(&self) -> (usize, usize) {\n        if self.last_adjustment.elapsed() < self.adjustment_interval {\n            return (\n                self.current_threads.load(Ordering::Relaxed),\n                self.current_batch_size.load(Ordering::Relaxed)\n            );\n        }\n        \n        // Get current metrics\n        let summary = self.metrics.summary();\n        let current_rate = /* parse docs_per_sec from summary */;\n        \n        // Adjust parameters based on performance\n        let new_threads = /* calculate optimal thread count */;\n        let new_batch_size = /* calculate optimal batch size */;\n        \n        // Update current values\n        self.current_threads.store(new_threads, Ordering::Relaxed);\n        self.current_batch_size.store(new_batch_size, Ordering::Relaxed);\n        \n        (new_threads, new_batch_size)\n    }\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Dynamic parameter tuning\nimport time\nimport threading\nimport os\n\nclass DynamicTuner:\n    def __init__(self, metrics, min_threads=1, max_threads=None, \n                 min_batch_size=100, max_batch_size=100000):\n        self.metrics = metrics\n        self.last_adjustment = time.time()\n        self.adjustment_interval = 30  # seconds\n        \n        # Thread count limits\n        self.min_threads = min_threads\n        self.max_threads = max_threads or os.cpu_count()\n        \n        # Batch size limits\n        self.min_batch_size = min_batch_size\n        self.max_batch_size = max_batch_size\n        \n        # Current values\n        self.current_threads = min(os.cpu_count(), self.max_threads)\n        self.current_batch_size = 1000\n        \n        # Lock for thread safety\n        self.lock = threading.RLock()\n    \n    def check_and_adjust(self):\n        with self.lock:\n            if time.time() - self.last_adjustment < self.adjustment_interval:\n                return self.current_threads, self.current_batch_size\n            \n            # Get current metrics\n            summary = self.metrics.summary()\n            current_rate = summary['docs_per_sec']\n            \n            # Adjust thread count based on CPU usage\n            # This is a simple heuristic and could be improved\n            new_threads = self.current_threads\n            if summary['processing_time'] > summary['indexing_time'] * 1.5:\n                # If processing is the bottleneck, increase threads\n                new_threads = min(self.current_threads + 2, self.max_threads)\n            elif summary['indexing_time'] > summary['processing_time'] * 1.5:\n                # If indexing is the bottleneck, possibly reduce threads\n                new_threads = max(self.current_threads - 1, self.min_threads)\n            \n            # Adjust batch size based on throughput\n            new_batch_size = self.current_batch_size\n            if current_rate < 100:  # Low throughput\n                # Try smaller batches for more frequent commits\n                new_batch_size = max(self.current_batch_size // 2, self.min_batch_size)\n            elif current_rate > 1000:  # High throughput\n                # Try larger batches for efficiency\n                new_batch_size = min(self.current_batch_size * 2, self.max_batch_size)\n            \n            # Update current values\n            self.current_threads = new_threads\n            self.current_batch_size = new_batch_size\n            self.last_adjustment = time.time()\n            \n            print(f\"Adjusted parameters: threads={new_threads}, batch_size={new_batch_size}\")\n            return new_threads, new_batch_size\n"})}),"\n",(0,s.jsx)(n.h2,{id:"end-to-end-pipeline-implementation",children:"End-to-End Pipeline Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"complete-rust-implementation",children:"Complete Rust Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Complete high-throughput indexing pipeline\nuse std::sync::{Arc, Mutex};\nuse std::thread;\nuse std::time::Instant;\nuse rayon::prelude::*;\n\npub struct IndexingPipeline {\n    content_store: Arc<dyn ContentStore + Send + Sync>,\n    index: Arc<Mutex<Index>>,\n    batch_size: usize,\n    num_threads: usize,\n    metrics: Arc<IndexingMetrics>,\n}\n\nimpl IndexingPipeline {\n    pub fn new(\n        content_store: Arc<dyn ContentStore + Send + Sync>,\n        index: Index,\n        batch_size: usize,\n        num_threads: usize\n    ) -> Self {\n        Self {\n            content_store,\n            index: Arc::new(Mutex::new(index)),\n            batch_size,\n            num_threads,\n            metrics: Arc::new(IndexingMetrics::new()),\n        }\n    }\n    \n    pub fn index_collection<I>(&self, doc_ids: I) -> Result<()>\n    where\n        I: IntoIterator<Item = DocId> + Send,\n        I::IntoIter: Send,\n    {\n        let doc_ids: Vec<_> = doc_ids.into_iter().collect();\n        let chunks: Vec<_> = doc_ids.chunks(self.batch_size).collect();\n        \n        // Enable bulk loading mode\n        {\n            let mut index = self.index.lock().unwrap();\n            if let Some(storage) = index.storage_as_rocksdb_mut() {\n                storage.set_bulk_loading_mode(true);\n            }\n        }\n        \n        println!("Starting indexing of {} documents in {} chunks", \n                 doc_ids.len(), chunks.len());\n        \n        // Process each chunk\n        chunks.par_iter()\n            .with_max_len(1)  // Process one chunk per thread\n            .for_each(|chunk| {\n                let start = Instant::now();\n                \n                // Retrieve documents for this chunk\n                let processing_start = Instant::now();\n                let documents = match self.content_store.batch_get_documents(chunk) {\n                    Ok(docs) => docs,\n                    Err(e) => {\n                        eprintln!("Error retrieving documents: {}", e);\n                        return;\n                    }\n                };\n                let processing_time = processing_start.elapsed();\n                \n                // Count tokens\n                let token_count = documents.iter()\n                    .map(|(_, content, _)| content.split_whitespace().count())\n                    .sum();\n                \n                // Index the documents\n                let indexing_start = Instant::now();\n                let mut index_guard = match self.index.lock() {\n                    Ok(guard) => guard,\n                    Err(e) => {\n                        eprintln!("Error acquiring index lock: {}", e);\n                        return;\n                    }\n                };\n                \n                if let Err(e) = index_guard.add_documents_batch(&documents) {\n                    eprintln!("Error indexing documents: {}", e);\n                }\n                \n                let indexing_time = indexing_start.elapsed();\n                \n                // Record metrics\n                self.metrics.record_batch(\n                    documents.len(),\n                    token_count,\n                    processing_time,\n                    indexing_time\n                );\n                \n                println!("Processed chunk in {:?}: {} documents, {} tokens",\n                         start.elapsed(), documents.len(), token_count);\n            });\n        \n        // Disable bulk loading mode and optimize\n        let mut index = self.index.lock().unwrap();\n        if let Some(storage) = index.storage_as_rocksdb_mut() {\n            storage.set_bulk_loading_mode(false);\n        }\n        \n        // Print final metrics\n        println!("{}", self.metrics.summary());\n        \n        Ok(())\n    }\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"complete-python-implementation",children:"Complete Python Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Complete high-throughput indexing pipeline\nimport concurrent.futures\nimport threading\nimport time\nimport os\nimport math\n\nclass IndexingPipeline:\n    def __init__(self, content_store, index, batch_size=1000, num_threads=None):\n        self.content_store = content_store\n        self.index = index\n        self.batch_size = batch_size\n        self.num_threads = num_threads or os.cpu_count()\n        self.metrics = IndexingMetrics()\n    \n    def index_collection(self, doc_ids):\n        # Convert to list if it\'s an iterator\n        doc_ids = list(doc_ids)\n        \n        # Split into chunks\n        chunk_size = self.batch_size\n        chunks = [doc_ids[i:i+chunk_size] for i in range(0, len(doc_ids), chunk_size)]\n        \n        print(f"Starting indexing of {len(doc_ids)} documents in {len(chunks)} chunks")\n        \n        # Process chunks in parallel\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n            futures = [executor.submit(self._process_chunk, chunk) for chunk in chunks]\n            \n            # Wait for all futures to complete\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    future.result()\n                except Exception as e:\n                    print(f"Error processing chunk: {e}")\n        \n        # Print final metrics\n        self.metrics.print_summary()\n    \n    def _process_chunk(self, chunk):\n        start_time = time.time()\n        \n        # Retrieve documents for this chunk\n        processing_start = time.time()\n        try:\n            documents = self.content_store.batch_get_documents(chunk)\n        except Exception as e:\n            print(f"Error retrieving documents: {e}")\n            return\n        \n        processing_time = time.time() - processing_start\n        \n        # Count tokens (simple whitespace tokenization)\n        token_count = sum(len(content.split()) for _, content, _ in documents)\n        \n        # Convert to format expected by index\n        batch = [(doc_id, {"content": content, **metadata}) for doc_id, content, metadata in documents]\n        \n        # Index the documents\n        indexing_start = time.time()\n        self.index.add_documents_with_fields_parallel(batch)\n        indexing_time = time.time() - indexing_start\n        \n        # Record metrics\n        self.metrics.record_batch(\n            len(documents),\n            token_count,\n            processing_time,\n            indexing_time\n        )\n        \n        elapsed = time.time() - start_time\n        print(f"Processed chunk in {elapsed:.2f}s: {len(documents)} documents, {token_count} tokens")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"By implementing the strategies and patterns outlined in this guide, you can achieve maximum indexing throughput while maintaining separation between your inverted index and content storage. The key principles are:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize RocksDB"})," - Use WriteBatch and bulk loading mode"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Process in Batches"})," - Consolidate operations for efficiency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallelize"})," - Use multiple threads for document retrieval and processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cache Strategically"})," - Use memory efficiently for frequently accessed content"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor Performance"})," - Collect metrics and adjust parameters dynamically"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Remember that the optimal configuration will depend on your specific hardware, data characteristics, and requirements. Use the performance monitoring tools to identify bottlenecks and tune accordingly."}),"\n",(0,s.jsx)(n.p,{children:"For further reading, refer to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/docs/rocksdb_optimization",children:"RocksDB Optimization Guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/docs/performance",children:"Performance Tuning"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/docs/memory-management",children:"Memory Management"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/docs/parallel-indexing",children:"Parallel Indexing"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function c(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);