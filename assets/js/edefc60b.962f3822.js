"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[3624],{3739:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"performance","title":"performance","description":"Fast-Inverted-Index is designed for high performance out of the box, but you can further optimize it for specific use cases. This guide covers various performance tuning strategies.","source":"@site/docs/performance.md","sourceDirName":".","slug":"/performance","permalink":"/fast-inverted-index-docs/docs/performance","draft":false,"unlisted":false,"editUrl":"https://github.com/username/fast-inverted-index/tree/main/docusaurus/docs/performance.md","tags":[],"version":"current","frontMatter":{"id":"performance","title":"performance","sidebar_label":"performance"},"sidebar":"docs","previous":{"title":"ouservauility","permalink":"/fast-inverted-index-docs/docs/observability"},"next":{"title":"testing","permalink":"/fast-inverted-index-docs/docs/testing"}}');var s=i(4848),r=i(8453);const a={id:"performance",title:"performance",sidebar_label:"performance"},o="Performance Tuning",c={},l=[{value:"Configuration Parameters",id:"configuration-parameters",level:2},{value:"Dictionary Configuration",id:"dictionary-configuration",level:3},{value:"Cache Size",id:"cache-size",level:3},{value:"Cache TTL",id:"cache-ttl",level:3},{value:"In-Memory Mode",id:"in-memory-mode",level:3},{value:"Position Storage",id:"position-storage",level:3},{value:"Bulk Operations",id:"bulk-operations",level:2},{value:"Bulk Indexing",id:"bulk-indexing",level:3},{value:"Parallel Indexing",id:"parallel-indexing",level:3},{value:"Query Optimization",id:"query-optimization",level:2},{value:"Query Construction",id:"query-construction",level:3},{value:"Field Boosting",id:"field-boosting",level:3},{value:"Term Frequency Impact",id:"term-frequency-impact",level:3},{value:"Storage Optimization",id:"storage-optimization",level:2},{value:"RocksDB Settings",id:"rocksdb-settings",level:3},{value:"Compression Settings",id:"compression-settings",level:3},{value:"Memory Management",id:"memory-management",level:2},{value:"Document Size",id:"document-size",level:3},{value:"Metadata Size",id:"metadata-size",level:3},{value:"Monitoring and Metrics",id:"monitoring-and-metrics",level:2},{value:"Track Index Statistics",id:"track-index-statistics",level:3},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Scaling Strategies",id:"scaling-strategies",level:2},{value:"Vertical Scaling",id:"vertical-scaling",level:3},{value:"Horizontal Scaling",id:"horizontal-scaling",level:3},{value:"Advanced Optimization",id:"advanced-optimization",level:2},{value:"Custom Tokenization",id:"custom-tokenization",level:3},{value:"Result Caching",id:"result-caching",level:3},{value:"Parallel Processing",id:"parallel-processing",level:2},{value:"Production Readiness Checklist",id:"production-readiness-checklist",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"performance-tuning",children:"Performance Tuning"})}),"\n",(0,s.jsx)(n.p,{children:"Fast-Inverted-Index is designed for high performance out of the box, but you can further optimize it for specific use cases. This guide covers various performance tuning strategies."}),"\n",(0,s.jsx)(n.h2,{id:"configuration-parameters",children:"Configuration Parameters"}),"\n",(0,s.jsx)(n.h3,{id:"dictionary-configuration",children:"Dictionary Configuration"}),"\n",(0,s.jsx)(n.p,{children:"The hybrid dictionary can be configured for optimal performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Configure the hybrid dictionary\nindex = Index.builder() \\\n    .with_dictionary_config({\n        "hot_terms_capacity": 20000,           # Size of hot terms layer\n        "hot_promotion_threshold": 100,         # Access count to promote to hot layer\n        "hot_demotion_threshold": 10,           # Access count below which hot terms are demoted\n        "medium_promotion_threshold": 5,         # Access count to promote from cold to medium\n        "medium_demotion_threshold": 1,          # Access count below which medium terms are demoted\n        "optimization_interval_secs": 3600       # How often to optimize the dictionary\n    }) \\\n    .build()\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Configure the hybrid dictionary\nlet index = IndexBuilder::new()\n    .with_dictionary_config(HybridDictionaryConfig {\n        hot_terms_capacity: 20000,\n        hot_promotion_threshold: 100,\n        hot_demotion_threshold: 10,\n        medium_promotion_threshold: 5,\n        medium_demotion_threshold: 1,\n        optimization_interval_secs: 3600,\n    })\n    .build()?;\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Guidelines:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"For small indices (< 100K documents): Hot terms capacity of 1,000-5,000"}),"\n",(0,s.jsx)(n.li,{children:"For medium indices (100K-1M documents): Hot terms capacity of 5,000-20,000"}),"\n",(0,s.jsx)(n.li,{children:"For large indices (> 1M documents): Hot terms capacity of 20,000-100,000"}),"\n",(0,s.jsx)(n.li,{children:"Adjust promotion/demotion thresholds based on query patterns"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cache-size",children:"Cache Size"}),"\n",(0,s.jsx)(n.p,{children:"The LRU cache size impacts query performance significantly:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Large cache for better performance\nindex = Index(cache_size=500000)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Large cache for better performance\nlet index = IndexBuilder::new()\n    .with_cache_size(500000)\n    .build()?;\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Guidelines:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"For small indices (< 100K documents): 10,000-50,000 cache entries"}),"\n",(0,s.jsx)(n.li,{children:"For medium indices (100K-1M documents): 50,000-500,000 cache entries"}),"\n",(0,s.jsx)(n.li,{children:"For large indices (> 1M documents): 500,000+ cache entries"}),"\n",(0,s.jsx)(n.li,{children:"Monitor memory usage and adjust accordingly"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cache-ttl",children:"Cache TTL"}),"\n",(0,s.jsx)(n.p,{children:"Set an appropriate time-to-live for cached entries:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - 1 hour TTL\nindex = Index(cache_ttl_secs=3600)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - 1 hour TTL\nuse std::time::Duration;\nlet index = IndexBuilder::new()\n    .with_cache_ttl(Duration::from_secs(3600))\n    .build()?;\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Guidelines:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"For mostly static data: Longer TTL (hours)"}),"\n",(0,s.jsx)(n.li,{children:"For frequently changing data: Shorter TTL (minutes)"}),"\n",(0,s.jsx)(n.li,{children:"For real-time applications: Very short TTL (seconds)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"in-memory-mode",children:"In-Memory Mode"}),"\n",(0,s.jsx)(n.p,{children:"For maximum performance at the cost of persistence:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - In-memory mode\nindex = Index(in_memory=True)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - In-memory mode\nlet index = IndexBuilder::new()\n    .with_in_memory(true)\n    .build()?;\n"})}),"\n",(0,s.jsx)(n.p,{children:"This mode is beneficial for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Testing and development"}),"\n",(0,s.jsx)(n.li,{children:"Temporary indices"}),"\n",(0,s.jsx)(n.li,{children:"Short-lived applications"}),"\n",(0,s.jsx)(n.li,{children:"Scenarios where persistence is handled externally"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"position-storage",children:"Position Storage"}),"\n",(0,s.jsx)(n.p,{children:"Disable position storage if you don't need phrase queries:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Disable position storage\nindex = Index(store_positions=False)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Disable position storage\nlet index = IndexBuilder::new()\n    .with_store_positions(false)\n    .build()?;\n"})}),"\n",(0,s.jsx)(n.p,{children:"This can reduce index size and improve indexing speed significantly."}),"\n",(0,s.jsx)(n.h2,{id:"bulk-operations",children:"Bulk Operations"}),"\n",(0,s.jsx)(n.h3,{id:"bulk-indexing",children:"Bulk Indexing"}),"\n",(0,s.jsx)(n.p,{children:"For large document sets, use batch processing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Bulk indexing\nstart_time = time.time()\n\nfor i, doc in enumerate(documents):\n    index.add_document(i, doc["content"])\n    \n    # Periodically optimize\n    if i > 0 and i % 10000 == 0:\n        index.optimize()\n        print(f"Indexed {i} documents...")\n\nelapsed_time = time.time() - start_time\nprint(f"Indexed {len(documents)} documents in {elapsed_time:.2f} seconds "\n      f"({len(documents) / elapsed_time:.2f} docs/sec)")\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Bulk indexing\nlet start = std::time::Instant::now();\nlet mut count = 0;\n\nfor (i, doc) in documents.iter().enumerate() {\n    index.add_document(i as u64, &doc.content)?;\n    count += 1;\n    \n    // Periodically optimize\n    if i > 0 && i % 10000 == 0 {\n        index.optimize()?;\n        println!("Indexed {} documents...", i);\n    }\n}\n\nlet elapsed = start.elapsed();\nprintln!("Indexed {} documents in {:.2?} ({:.2} docs/sec)",\n         count, elapsed, count as f64 / elapsed.as_secs_f64());\n'})}),"\n",(0,s.jsx)(n.h3,{id:"parallel-indexing",children:"Parallel Indexing"}),"\n",(0,s.jsx)(n.p,{children:"For multi-core systems, distribute indexing across processes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Parallel indexing\nfrom concurrent.futures import ProcessPoolExecutor\nimport os\n\ndef index_document_batch(batch):\n    # Create a separate index for each process\n    idx = Index(in_memory=True)\n    for doc in batch:\n        idx.add_document(doc["id"], doc["content"])\n    return idx.stats()\n\n# Split documents into batches\ndef chunks(lst, n):\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\n# Number of processes\nnum_processes = os.cpu_count()\nprint(f"Using {num_processes} processes")\n\n# Batch size\nbatch_size = max(1, len(documents) // num_processes)\nbatches = list(chunks(documents, batch_size))\n\n# Process in parallel\nwith ProcessPoolExecutor(max_workers=num_processes) as executor:\n    futures = [executor.submit(index_document_batch, batch) for batch in batches]\n    results = [future.result() for future in futures]\n\n# Show results\ntotal_docs = sum(r["document_count"] for r in results)\ntotal_terms = sum(r["term_count"] for r in results)\nprint(f"Indexed {total_docs} documents with {total_terms} terms")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"query-optimization",children:"Query Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"query-construction",children:"Query Construction"}),"\n",(0,s.jsx)(n.p,{children:"Choose the right query type:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Single term (fastest)\nresults = index.term_query("word")\n\n# AND query (faster than OR)\nresults = index.and_query(["word1", "word2"])\n\n# OR query (slower)\nresults = index.or_query(["word1", "word2"])\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Single term (fastest)\nlet query = Query::term("word");\n\n// AND query (faster than OR)\nlet query = Query::and(vec!["word1".to_string(), "word2".to_string()]);\n\n// OR query (slower)\nlet query = Query::or(vec!["word1".to_string(), "word2".to_string()]);\n'})}),"\n",(0,s.jsx)(n.h3,{id:"field-boosting",children:"Field Boosting"}),"\n",(0,s.jsx)(n.p,{children:"Optimize relevance with field boosting:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Boost important fields\nresults = index.search_with_metadata(\n    "query terms",\n    ranking_method="bm25",\n    boost_fields={\n        "title": 3.0,     # Title is most important\n        "tags": 2.0,      # Tags are very relevant\n        "summary": 1.5,   # Summary is somewhat relevant\n        # Content has default weight of 1.0\n    }\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Boost important fields\nlet mut field_boosts = std::collections::HashMap::new();\nfield_boosts.insert("title".to_string(), 3.0);\nfield_boosts.insert("tags".to_string(), 2.0);\nfield_boosts.insert("summary".to_string(), 1.5);\n\nlet query = Query::parse("query terms");\nlet results = index.search_bm25(&query, Some(field_boosts))?;\n'})}),"\n",(0,s.jsx)(n.h3,{id:"term-frequency-impact",children:"Term Frequency Impact"}),"\n",(0,s.jsx)(n.p,{children:"Be aware of term frequency impact:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Common terms (high frequency) get lower relevance scores"}),"\n",(0,s.jsx)(n.li,{children:"Rare terms (low frequency) get higher relevance scores"}),"\n",(0,s.jsxs)(n.li,{children:["For exact matching, consider using ",(0,s.jsx)(n.code,{children:"term_query"})," instead of ranked search"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"storage-optimization",children:"Storage Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"rocksdb-settings",children:"RocksDB Settings"}),"\n",(0,s.jsx)(n.p,{children:"For persistent storage, optimize RocksDB parameters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Custom RocksDB options\nuse rocksdb::{Options, DB};\n\nlet mut options = Options::default();\noptions.set_max_open_files(1000);\noptions.set_use_fsync(false);\noptions.set_bytes_per_sync(8388608); // 8MB\noptions.set_compression_type(rocksdb::DBCompressionType::Lz4);\noptions.set_compaction_style(rocksdb::DBCompactionStyle::Level);\noptions.set_write_buffer_size(64 * 1024 * 1024); // 64MB\noptions.set_max_write_buffer_number(4);\noptions.set_target_file_size_base(64 * 1024 * 1024); // 64MB\n\n// Currently these advanced options need to be set through environment variables\n// until a more flexible API is available\nstd::env::set_var("ROCKSDB_MAX_OPEN_FILES", "1000");\nstd::env::set_var("ROCKSDB_BYTES_PER_SYNC", "8388608");\n'})}),"\n",(0,s.jsx)(n.h3,{id:"compression-settings",children:"Compression Settings"}),"\n",(0,s.jsx)(n.p,{children:"Balance between storage size and performance:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Varint compression is used by default for efficient storage"}),"\n",(0,s.jsx)(n.li,{children:"For maximum performance, use the in-memory mode"}),"\n",(0,s.jsx)(n.li,{children:"For better compression, use the persistent storage with a smaller block size"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"memory-management",children:"Memory Management"}),"\n",(0,s.jsx)(n.h3,{id:"document-size",children:"Document Size"}),"\n",(0,s.jsx)(n.p,{children:"Keep document sizes reasonable:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Break very large documents into smaller chunks"}),"\n",(0,s.jsx)(n.li,{children:"If using positions, limit content to a reasonable size (e.g., < 100KB per document)"}),"\n",(0,s.jsx)(n.li,{children:"For very large text, consider extracting key sections or summarizing"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Handle large documents by chunking\ndef index_large_document(index, doc_id, content, chunk_size=50000):\n    """Index a large document by breaking it into chunks."""\n    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n    \n    # Index each chunk with the same document ID\n    for i, chunk in enumerate(chunks):\n        metadata = {\n            "title": f"Document {doc_id} - Chunk {i+1}",\n            "parent_id": doc_id,\n            "chunk_index": i,\n            "total_chunks": len(chunks)\n        }\n        index.add_document(f"{doc_id}_{i}", chunk, metadata)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"metadata-size",children:"Metadata Size"}),"\n",(0,s.jsx)(n.p,{children:"Keep metadata fields concise:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Large arrays (e.g., thousands of tags) can impact performance"}),"\n",(0,s.jsx)(n.li,{children:"Keep string fields to a reasonable size"}),"\n",(0,s.jsx)(n.li,{children:"Use references to external storage for large binary data"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-and-metrics",children:"Monitoring and Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"track-index-statistics",children:"Track Index Statistics"}),"\n",(0,s.jsx)(n.p,{children:"Monitor index statistics to identify performance issues:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Monitor statistics\nstats = index.stats()\nprint(f\"Documents: {stats['document_count']}\")\nprint(f\"Terms: {stats['term_count']}\")\nprint(f\"Cache hit rate: {stats['cache_hit_rate']}\")\nprint(f\"Avg query time: {stats['avg_query_time_ms']} ms\")\nprint(f\"Avg indexing time: {stats['avg_indexing_time_ms']} ms\")\n\n# Dictionary statistics\nif 'dictionary' in stats:\n    dict_stats = stats['dictionary']\n    print(\"\\nDictionary Statistics:\")\n    print(f\"Total terms: {dict_stats['total_terms']}\")\n    print(f\"Hot terms: {dict_stats['tier_distribution']['hot']}\")\n    print(f\"Medium terms: {dict_stats['tier_distribution']['medium']}\")\n    print(f\"Cold terms: {dict_stats['tier_distribution']['cold']}\")\n    print(f\"Dictionary hit rate: {dict_stats['hit_rate']:.2f}\")\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'// Rust - Monitor statistics\nlet stats = index.stats()?;\nprintln!("Documents: {}", stats.document_count);\nprintln!("Terms: {}", stats.term_count);\nprintln!("Cache hit rate: {}", stats.cache_hit_rate);\nprintln!("Avg query time: {} ms", stats.avg_query_time_ms);\nprintln!("Avg indexing time: {} ms", stats.avg_indexing_time_ms);\n\n// Dictionary statistics\nif let Some(dict_stats) = stats.dictionary_stats {\n    println!("\\nDictionary Statistics:");\n    println!("Total terms: {}", dict_stats.total_terms);\n    println!("Hot terms: {}", dict_stats.hot_terms);\n    println!("Medium terms: {}", dict_stats.medium_terms);\n    println!("Cold terms: {}", dict_stats.cold_terms);\n    println!("Dictionary hit rate: {:.2}", dict_stats.hit_rate);\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,s.jsx)(n.p,{children:"Implement performance tests for your workload:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Python - Simple performance test\nimport time\n\n# Test indexing performance\ndoc_count = 10000\nwords_per_doc = 100\ndocs = [" ".join(random.choice(words) for _ in range(words_per_doc)) \n        for _ in range(doc_count)]\n\nstart_time = time.time()\nfor i, content in enumerate(docs):\n    index.add_document(i, content)\nelapsed = time.time() - start_time\n\nprint(f"Indexed {doc_count} documents in {elapsed:.2f} seconds")\nprint(f"Rate: {doc_count / elapsed:.2f} docs/sec")\n\n# Test query performance\nquery_count = 100\nqueries = [random.choice(words) for _ in range(query_count)]\n\nstart_time = time.time()\nfor query in queries:\n    index.term_query(query)\nelapsed = time.time() - start_time\n\nprint(f"Executed {query_count} queries in {elapsed:.2f} seconds")\nprint(f"Rate: {query_count / elapsed:.2f} queries/sec")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"scaling-strategies",children:"Scaling Strategies"}),"\n",(0,s.jsx)(n.h3,{id:"vertical-scaling",children:"Vertical Scaling"}),"\n",(0,s.jsx)(n.p,{children:"Optimize for a single machine:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Increase cache size for better performance"}),"\n",(0,s.jsx)(n.li,{children:"Use in-memory mode if data fits in RAM"}),"\n",(0,s.jsx)(n.li,{children:"Optimize RocksDB settings for your storage"}),"\n",(0,s.jsxs)(n.li,{children:["Periodically run ",(0,s.jsx)(n.code,{children:"optimize()"})," to compact the index"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"horizontal-scaling",children:"Horizontal Scaling"}),"\n",(0,s.jsx)(n.p,{children:"For very large datasets, consider these strategies:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Sharding by Document ID"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Split documents across multiple indices based on ID ranges"}),"\n",(0,s.jsx)(n.li,{children:"Query each shard and merge results"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Sharding by Term"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Split terms across multiple indices"}),"\n",(0,s.jsx)(n.li,{children:"Route queries to appropriate shards based on terms"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Replication"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Maintain identical copies of the index on multiple machines"}),"\n",(0,s.jsx)(n.li,{children:"Use for read scaling and fault tolerance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Example of a simple document-based sharding strategy:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ShardedIndex:\n    def __init__(self, shard_count=4, storage_base_path=None):\n        """Initialize a sharded index."""\n        self.shard_count = shard_count\n        self.shards = []\n        \n        for i in range(shard_count):\n            storage_path = None\n            if storage_base_path:\n                storage_path = f"{storage_base_path}/shard_{i}"\n            \n            self.shards.append(Index(storage_path=storage_path))\n    \n    def add_document(self, doc_id, content, metadata=None):\n        """Add a document to the appropriate shard."""\n        shard_index = doc_id % self.shard_count\n        self.shards[shard_index].add_document(doc_id, content, metadata)\n    \n    def search(self, query):\n        """Search across all shards and merge results."""\n        all_results = []\n        \n        for shard in self.shards:\n            results = shard.search(query, ranking_method="bm25")\n            all_results.extend(results)\n        \n        # Sort by score (higher is better)\n        all_results.sort(key=lambda x: x[1], reverse=True)\n        return all_results\n'})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-optimization",children:"Advanced Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"custom-tokenization",children:"Custom Tokenization"}),"\n",(0,s.jsx)(n.p,{children:"For specialized domains, implement custom tokenization:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Custom tokenization example\ndef custom_tokenize(text):\n    \"\"\"Custom tokenization for specialized domains.\"\"\"\n    # Remove common noise\n    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n    \n    # Handle domain-specific terms\n    text = re.sub(r'c\\+\\+', 'cpp', text)\n    text = re.sub(r'\\.net', 'dotnet', text)\n    \n    # Split and filter\n    tokens = text.split()\n    tokens = [t for t in tokens if len(t) > 1]\n    \n    return tokens\n\n# Use with a pre-processor\nfor doc in documents:\n    processed_content = ' '.join(custom_tokenize(doc['content']))\n    index.add_document(doc['id'], processed_content)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"result-caching",children:"Result Caching"}),"\n",(0,s.jsx)(n.p,{children:"For frequently accessed queries, implement a result cache:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CachedSearchEngine:\n    def __init__(self, index, cache_size=1000):\n        """Initialize a search engine with query result caching."""\n        self.index = index\n        self.cache = {}\n        self.cache_size = cache_size\n        self.cache_hits = 0\n        self.cache_misses = 0\n    \n    def search(self, query, force_refresh=False):\n        """Search with caching."""\n        # Normalize the query for cache key\n        cache_key = query.lower().strip()\n        \n        # Check cache unless refresh is forced\n        if not force_refresh and cache_key in self.cache:\n            self.cache_hits += 1\n            return self.cache[cache_key]\n        \n        # Execute the query\n        self.cache_misses += 1\n        results = self.index.search(query, ranking_method="bm25")\n        \n        # Cache the results\n        self.cache[cache_key] = results\n        \n        # Maintain cache size\n        if len(self.cache) > self.cache_size:\n            # Simple LRU: remove the first item (oldest)\n            self.cache.pop(next(iter(self.cache)))\n        \n        return results\n    \n    def cache_stats(self):\n        """Get cache statistics."""\n        total = self.cache_hits + self.cache_misses\n        hit_rate = self.cache_hits / total if total > 0 else 0\n        return {\n            "size": len(self.cache),\n            "capacity": self.cache_size,\n            "hits": self.cache_hits,\n            "misses": self.cache_misses,\n            "hit_rate": hit_rate\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"parallel-processing",children:"Parallel Processing"}),"\n",(0,s.jsx)(n.p,{children:"For significantly improved indexing performance on multi-core systems, use the parallel indexing capabilities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python - Parallel document indexing\nindex.add_documents_parallel(documents, num_threads=None, batch_size=100)\n\n# Python - Parallel optimization\nindex.optimize_parallel()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"// Rust - Parallel document indexing\nlet config = ParallelIndexingConfig {\n    num_threads: 0,  // 0 = auto-detect\n    batch_size: 100,\n    channel_capacity: 16,\n};\n\nindex.add_documents_parallel(documents, Some(config))?;\n\n// Rust - Parallel optimization\nindex.optimize_parallel(None)?;\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For detailed information on parallel processing, see the ",(0,s.jsx)(n.a,{href:"/fast-inverted-index-docs/docs/parallel-indexing",children:"Parallel Indexing"})," guide."]}),"\n",(0,s.jsx)(n.h2,{id:"production-readiness-checklist",children:"Production Readiness Checklist"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configure appropriate cache size and TTL"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Enable parallel indexing for multi-core systems"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up periodic index optimization"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement monitoring for index statistics"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test query performance with realistic data volumes"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement appropriate error handling"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up backup and recovery procedures"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Consider sharding for very large datasets"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Tune RocksDB settings for persistent storage"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement result caching for common queries"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Monitor memory usage in production"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"By following these performance tuning guidelines, you can ensure that your Fast-Inverted-Index implementation delivers optimal performance for your specific use case."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);