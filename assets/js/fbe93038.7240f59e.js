"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[1133],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}},8800:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"testing","title":"testing","description":"This guide covers testing strategies for applications built with Fast-Inverted-Index. Following these testing practices will help ensure reliability and performance.","source":"@site/docs/testing.md","sourceDirName":".","slug":"/testing","permalink":"/docs/testing","draft":false,"unlisted":false,"editUrl":"https://github.com/username/fast-inverted-index/tree/main/docusaurus/docs/testing.md","tags":[],"version":"current","frontMatter":{"id":"testing","title":"testing","sidebar_label":"testing"},"sidebar":"docs","previous":{"title":"performance","permalink":"/docs/performance"}}');var r=t(4848),i=t(8453);const o={id:"testing",title:"testing",sidebar_label:"testing"},a="Testing Guide",d={},l=[{value:"Unit Testing",id:"unit-testing",level:2},{value:"Testing the Rust API",id:"testing-the-rust-api",level:3},{value:"Testing the Python API",id:"testing-the-python-api",level:3},{value:"Integration Testing",id:"integration-testing",level:2},{value:"Testing Search Functionality",id:"testing-search-functionality",level:3},{value:"Testing with Real Data",id:"testing-with-real-data",level:3},{value:"Performance Testing",id:"performance-testing",level:2},{value:"Benchmarking",id:"benchmarking",level:3},{value:"Load Testing",id:"load-testing",level:3},{value:"Mocking for Tests",id:"mocking-for-tests",level:2},{value:"Continuous Integration",id:"continuous-integration",level:2},{value:"Automated Testing",id:"automated-testing",level:3},{value:"Performance Regression Testing",id:"performance-regression-testing",level:3},{value:"Best Practices",id:"best-practices",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"testing-guide",children:"Testing Guide"})}),"\n",(0,r.jsx)(n.p,{children:"This guide covers testing strategies for applications built with Fast-Inverted-Index. Following these testing practices will help ensure reliability and performance."}),"\n",(0,r.jsx)(n.h2,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,r.jsx)(n.h3,{id:"testing-the-rust-api",children:"Testing the Rust API"}),"\n",(0,r.jsx)(n.p,{children:"For Rust projects, use the standard testing framework:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-rust",children:'#[cfg(test)]\nmod tests {\n    use fast_inverted_index::{IndexBuilder, Query};\n    use std::error::Error;\n\n    #[test]\n    fn test_basic_indexing_and_querying() -> Result<(), Box<dyn Error>> {\n        // Create an in-memory index for testing\n        let mut index = IndexBuilder::new()\n            .with_in_memory(true)\n            .build()?;\n        \n        // Add some test documents\n        index.add_document(1, "This is a test document")?;\n        index.add_document(2, "Another test with different content")?;\n        \n        // Test a simple query\n        let query = Query::term("test");\n        let results = index.query(&query)?;\n        \n        assert_eq!(results.doc_ids.len(), 2);\n        assert!(results.doc_ids.contains(&1));\n        assert!(results.doc_ids.contains(&2));\n        \n        // Test an AND query\n        let query = Query::and(vec![\n            "test".to_string(),\n            "different".to_string(),\n        ]);\n        let results = index.query(&query)?;\n        \n        assert_eq!(results.doc_ids.len(), 1);\n        assert!(results.doc_ids.contains(&2));\n        \n        Ok(())\n    }\n    \n    #[test]\n    fn test_term_suggestions() -> Result<(), Box<dyn Error>> {\n        let mut index = IndexBuilder::new()\n            .with_in_memory(true)\n            .build()?;\n        \n        index.add_document(1, "Testing term suggestions feature")?;\n        index.add_document(2, "Terms should be suggested correctly")?;\n        \n        let suggestions = index.suggest_terms("te")?;\n        \n        assert!(!suggestions.is_empty());\n        assert!(suggestions.iter().any(|s| s.term == "term" || s.term == "terms"));\n        assert!(suggestions.iter().any(|s| s.term == "test" || s.term == "testing"));\n        \n        Ok(())\n    }\n    \n    #[test]\n    fn test_document_removal() -> Result<(), Box<dyn Error>> {\n        let mut index = IndexBuilder::new()\n            .with_in_memory(true)\n            .build()?;\n        \n        // Add documents\n        index.add_document(1, "This is document one")?;\n        index.add_document(2, "This is document two")?;\n        \n        // Verify both documents are indexed\n        let query = Query::term("document");\n        let results = index.query(&query)?;\n        assert_eq!(results.doc_ids.len(), 2);\n        \n        // Remove one document\n        index.remove_document(1)?;\n        \n        // Verify only one document remains\n        let results = index.query(&query)?;\n        assert_eq!(results.doc_ids.len(), 1);\n        assert!(results.doc_ids.contains(&2));\n        \n        Ok(())\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"testing-the-python-api",children:"Testing the Python API"}),"\n",(0,r.jsx)(n.p,{children:"For Python projects, use pytest or unittest:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import unittest\nfrom fast_inverted_index import Index\n\nclass TestFastInvertedIndex(unittest.TestCase):\n    def setUp(self):\n        """Set up a test index before each test."""\n        self.index = Index(in_memory=True)\n        \n        # Add some documents\n        self.index.add_document(1, "This is document one for testing")\n        self.index.add_document(2, "Document two contains different words")\n        self.index.add_document(3, "The third document has more test content")\n    \n    def test_term_query(self):\n        """Test single term queries."""\n        results = self.index.term_query("document")\n        self.assertEqual(len(results), 3)\n        \n        results = self.index.term_query("one")\n        self.assertEqual(len(results), 1)\n        self.assertIn(1, results)\n        \n        results = self.index.term_query("nonexistent")\n        self.assertEqual(len(results), 0)\n    \n    def test_and_query(self):\n        """Test AND queries."""\n        results = self.index.and_query(["document", "test"])\n        self.assertEqual(len(results), 1)\n        self.assertIn(3, results)\n        \n        results = self.index.and_query(["document", "one"])\n        self.assertEqual(len(results), 1)\n        self.assertIn(1, results)\n    \n    def test_or_query(self):\n        """Test OR queries."""\n        results = self.index.or_query(["one", "two"])\n        self.assertEqual(len(results), 2)\n        self.assertIn(1, results)\n        self.assertIn(2, results)\n    \n    def test_document_removal(self):\n        """Test document removal."""\n        # Verify document exists\n        results = self.index.term_query("one")\n        self.assertEqual(len(results), 1)\n        \n        # Remove document\n        self.index.remove_document(1)\n        \n        # Verify it\'s gone\n        results = self.index.term_query("one")\n        self.assertEqual(len(results), 0)\n        \n        # Verify other documents still exist\n        results = self.index.term_query("document")\n        self.assertEqual(len(results), 2)\n    \n    def test_suggestions(self):\n        """Test term suggestions."""\n        suggestions = self.index.suggest_terms("do")\n        \n        # Verify document or documents is suggested\n        terms = [s["term"] for s in suggestions]\n        self.assertTrue(any(t.startswith("doc") for t in terms))\n    \n    def test_metadata(self):\n        """Test document metadata."""\n        # Add document with metadata\n        self.index.add_document(\n            doc_id=4,\n            content="Document with metadata",\n            metadata={\n                "title": "Test Title",\n                "author": "Test Author",\n                "tags": ["test", "metadata"]\n            }\n        )\n        \n        # Try to get document\n        try:\n            doc = self.index.get_document(4)\n            self.assertEqual(doc["title"], "Test Title")\n            self.assertEqual(doc["author"], "Test Author")\n            self.assertEqual(doc["tags"], ["test", "metadata"])\n        except Exception as e:\n            # If get_document fails in test environment, \n            # verify metadata was accepted without error\n            self.skipTest(f"get_document failed: {e}")\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,r.jsx)(n.h3,{id:"testing-search-functionality",children:"Testing Search Functionality"}),"\n",(0,r.jsx)(n.p,{children:"Test the complete search flow from indexing to results:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import unittest\nimport tempfile\nimport os\nimport shutil\nfrom fast_inverted_index import Index\n\nclass TestSearchFunctionality(unittest.TestCase):\n    def setUp(self):\n        """Set up a test index with test data."""\n        # Create a temporary directory\n        self.temp_dir = tempfile.mkdtemp()\n        \n        # Create an index with persistent storage\n        self.index = Index(storage_path=self.temp_dir)\n        \n        # Add test documents\n        self.documents = [\n            {\n                "id": 1,\n                "content": "Python is a programming language used for web development, \n                           data analysis, and artificial intelligence.",\n                "metadata": {\n                    "title": "Python Programming Language",\n                    "tags": ["python", "programming", "language"]\n                }\n            },\n            {\n                "id": 2,\n                "content": "Rust is a systems programming language focused on safety, \n                           speed, and concurrency.",\n                "metadata": {\n                    "title": "Rust Programming Language",\n                    "tags": ["rust", "programming", "language"]\n                }\n            },\n            {\n                "id": 3,\n                "content": "Fast-Inverted-Index is a high-performance search library \n                           implemented in Rust with Python bindings.",\n                "metadata": {\n                    "title": "Fast-Inverted-Index",\n                    "tags": ["search", "rust", "python"]\n                }\n            }\n        ]\n        \n        # Index the documents\n        for doc in self.documents:\n            self.index.add_document(\n                doc_id=doc["id"],\n                content=doc["content"],\n                metadata=doc["metadata"]\n            )\n    \n    def tearDown(self):\n        """Clean up temporary files."""\n        shutil.rmtree(self.temp_dir)\n    \n    def test_end_to_end_search(self):\n        """Test the complete search flow."""\n        # Test simple term search\n        results = self.index.search("programming", ranking_method="bm25")\n        self.assertGreaterEqual(len(results), 2)  # At least 2 documents\n        \n        # Get the document IDs from results\n        doc_ids = [doc_id for doc_id, _ in results]\n        \n        # Verify expected documents are found\n        self.assertIn(1, doc_ids)  # Python\n        self.assertIn(2, doc_ids)  # Rust\n        \n        # Test multi-term search\n        results = self.index.search("rust python", ranking_method="bm25")\n        doc_ids = [doc_id for doc_id, _ in results]\n        \n        # Document 3 should be most relevant as it contains both terms\n        self.assertIn(3, doc_ids)\n        \n        # Test field boosting\n        results = self.index.search_with_metadata(\n            "python",\n            ranking_method="bm25",\n            boost_fields={"title": 2.0}\n        )\n        \n        # Get document data from results\n        documents = [(doc["id"], doc["score"]) for doc in results]\n        doc_ids = [doc["id"] for doc in results]\n        \n        # Python in title should be boosted and ranked higher\n        self.assertIn(1, doc_ids)  # Python in title\n    \n    def test_persistence(self):\n        """Test index persistence."""\n        # Create a new index pointing to the same storage\n        new_index = Index(storage_path=self.temp_dir)\n        \n        # Verify the documents are accessible\n        results = new_index.term_query("python")\n        self.assertGreaterEqual(len(results), 1)\n        \n        # Verify stats show the correct document count\n        stats = new_index.stats()\n        self.assertEqual(stats["document_count"], len(self.documents))\n'})}),"\n",(0,r.jsx)(n.h3,{id:"testing-with-real-data",children:"Testing with Real Data"}),"\n",(0,r.jsx)(n.p,{children:"For more realistic testing, use a representative dataset:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import unittest\nimport csv\nimport tempfile\nimport shutil\nimport time\nfrom fast_inverted_index import Index\n\nclass TestWithRealData(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        """Set up once for all tests - load and index data."""\n        cls.temp_dir = tempfile.mkdtemp()\n        cls.index = Index(storage_path=cls.temp_dir)\n        \n        # Track indexing time\n        start_time = time.time()\n        \n        # Load and index data\n        doc_count = 0\n        with open(\'test_data.csv\', \'r\', encoding=\'utf-8\') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                cls.index.add_document(\n                    doc_id=int(row[\'id\']),\n                    content=row[\'content\'],\n                    metadata={\n                        \'title\': row[\'title\'],\n                        \'category\': row[\'category\'],\n                        \'tags\': row[\'tags\'].split(\',\') if row[\'tags\'] else []\n                    }\n                )\n                doc_count += 1\n                \n                # Limit data size for tests if needed\n                if doc_count >= 1000:\n                    break\n        \n        cls.doc_count = doc_count\n        cls.indexing_time = time.time() - start_time\n        print(f"Indexed {doc_count} documents in {cls.indexing_time:.2f} seconds")\n    \n    @classmethod\n    def tearDownClass(cls):\n        """Clean up after all tests."""\n        shutil.rmtree(cls.temp_dir)\n    \n    def test_query_performance(self):\n        """Test query performance with real data."""\n        # List of test queries\n        test_queries = [\n            "python programming",\n            "machine learning",\n            "data science",\n            "web development",\n            "software engineering"\n        ]\n        \n        for query in test_queries:\n            start_time = time.time()\n            results = self.index.search(query, ranking_method="bm25")\n            query_time = time.time() - start_time\n            \n            print(f"Query \'{query}\' returned {len(results)} results in {query_time*1000:.2f}ms")\n            \n            # Assert reasonable performance - adjust threshold as needed\n            self.assertLess(query_time, 0.1, f"Query \'{query}\' took too long: {query_time:.4f}s")\n    \n    def test_large_result_set(self):\n        """Test handling of large result sets."""\n        # Choose a common term likely to appear in many documents\n        common_term = "the"\n        \n        start_time = time.time()\n        results = self.index.term_query(common_term)\n        query_time = time.time() - start_time\n        \n        print(f"Query for common term \'{common_term}\' returned {len(results)} "\n              f"results in {query_time*1000:.2f}ms")\n        \n        # Assert we can handle large result sets efficiently\n        self.assertLess(query_time, 0.2, "Large result set handling is too slow")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,r.jsx)(n.h3,{id:"benchmarking",children:"Benchmarking"}),"\n",(0,r.jsx)(n.p,{children:"Create benchmarks for critical operations:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport random\nimport statistics\nfrom fast_inverted_index import Index\n\ndef benchmark_operation(operation, iterations=100):\n    """Benchmark an operation over multiple iterations."""\n    times = []\n    \n    for _ in range(iterations):\n        start_time = time.time()\n        result = operation()\n        elapsed = time.time() - start_time\n        times.append(elapsed)\n    \n    # Calculate statistics\n    avg_time = statistics.mean(times)\n    min_time = min(times)\n    max_time = max(times)\n    p95_time = sorted(times)[int(iterations * 0.95)]\n    \n    return {\n        "avg_ms": avg_time * 1000,\n        "min_ms": min_time * 1000,\n        "max_ms": max_time * 1000,\n        "p95_ms": p95_time * 1000,\n        "iterations": iterations\n    }\n\n# Set up an index with test data\nindex = Index(in_memory=True)\n\n# Add a reasonable number of documents\nprint("Adding documents...")\nfor i in range(10000):\n    index.add_document(\n        i, \n        f"Document {i} with some random words {\' \'.join(random.sample(words, 20))}"\n    )\n\n# Benchmark queries\nprint("\\nBenchmarking term queries...")\ncommon_term_stats = benchmark_operation(\n    lambda: index.term_query("document"),\n    iterations=1000\n)\nprint(f"Common term query: Avg={common_term_stats[\'avg_ms\']:.2f}ms, "\n      f"P95={common_term_stats[\'p95_ms\']:.2f}ms")\n\nrare_term_stats = benchmark_operation(\n    lambda: index.term_query(random.choice(rare_words)),\n    iterations=1000\n)\nprint(f"Rare term query: Avg={rare_term_stats[\'avg_ms\']:.2f}ms, "\n      f"P95={rare_term_stats[\'p95_ms\']:.2f}ms")\n\n# Benchmark AND queries\nprint("\\nBenchmarking AND queries...")\nand_query_stats = benchmark_operation(\n    lambda: index.and_query(["document", random.choice(words)]),\n    iterations=1000\n)\nprint(f"AND query: Avg={and_query_stats[\'avg_ms\']:.2f}ms, "\n      f"P95={and_query_stats[\'p95_ms\']:.2f}ms")\n\n# Benchmark OR queries\nprint("\\nBenchmarking OR queries...")\nor_query_stats = benchmark_operation(\n    lambda: index.or_query(["document", random.choice(words)]),\n    iterations=1000\n)\nprint(f"OR query: Avg={or_query_stats[\'avg_ms\']:.2f}ms, "\n      f"P95={or_query_stats[\'p95_ms\']:.2f}ms")\n\n# Benchmark suggestions\nprint("\\nBenchmarking suggestions...")\nsuggestion_stats = benchmark_operation(\n    lambda: index.suggest_terms(random.choice(word_prefixes)),\n    iterations=1000\n)\nprint(f"Term suggestions: Avg={suggestion_stats[\'avg_ms\']:.2f}ms, "\n      f"P95={suggestion_stats[\'p95_ms\']:.2f}ms")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"load-testing",children:"Load Testing"}),"\n",(0,r.jsx)(n.p,{children:"Test how the system performs under load:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import threading\nimport time\nimport random\nimport queue\nfrom fast_inverted_index import Index\n\nclass LoadTest:\n    def __init__(self, index, duration=60, threads=10):\n        """Initialize a load test."""\n        self.index = index\n        self.duration = duration  # seconds\n        self.threads = threads\n        self.stop_event = threading.Event()\n        self.queries_completed = 0\n        self.errors = 0\n        self.query_times = []\n        self.results_queue = queue.Queue()\n    \n    def query_worker(self, queries):\n        """Worker thread that executes queries."""\n        while not self.stop_event.is_set():\n            query = random.choice(queries)\n            \n            try:\n                start_time = time.time()\n                \n                # Execute the query\n                if random.random() < 0.6:  # 60% term queries\n                    self.index.term_query(query)\n                elif random.random() < 0.8:  # 20% AND queries\n                    terms = [query, random.choice(queries)]\n                    self.index.and_query(terms)\n                else:  # 20% suggestions\n                    self.index.suggest_terms(query[:3])\n                \n                elapsed = time.time() - start_time\n                \n                # Record the result\n                self.results_queue.put(("success", elapsed))\n                \n            except Exception as e:\n                self.results_queue.put(("error", str(e)))\n    \n    def stats_collector(self):\n        """Collect and process results from worker threads."""\n        while not self.stop_event.is_set() or not self.results_queue.empty():\n            try:\n                result_type, data = self.results_queue.get(timeout=0.1)\n                \n                if result_type == "success":\n                    self.queries_completed += 1\n                    self.query_times.append(data)\n                else:\n                    self.errors += 1\n                    \n                self.results_queue.task_done()\n            except queue.Empty:\n                continue\n    \n    def run(self, queries):\n        """Run the load test."""\n        print(f"Starting load test with {self.threads} threads for {self.duration} seconds")\n        \n        # Start worker threads\n        workers = []\n        for _ in range(self.threads):\n            worker = threading.Thread(\n                target=self.query_worker,\n                args=(queries,)\n            )\n            worker.daemon = True\n            worker.start()\n            workers.append(worker)\n        \n        # Start stats collector\n        collector = threading.Thread(target=self.stats_collector)\n        collector.daemon = True\n        collector.start()\n        \n        # Run for the specified duration\n        start_time = time.time()\n        try:\n            while time.time() - start_time < self.duration:\n                time.sleep(1)\n                \n                # Print progress\n                elapsed = time.time() - start_time\n                qps = self.queries_completed / elapsed if elapsed > 0 else 0\n                print(f"\\rRunning: {elapsed:.1f}s, Queries: {self.queries_completed}, "\n                      f"QPS: {qps:.1f}, Errors: {self.errors}", end="")\n        \n        finally:\n            # Stop all threads\n            self.stop_event.set()\n            \n            # Wait for threads to finish\n            for worker in workers:\n                worker.join(timeout=2)\n            collector.join(timeout=2)\n            \n            # Print final stats\n            actual_duration = time.time() - start_time\n            qps = self.queries_completed / actual_duration if actual_duration > 0 else 0\n            \n            if self.query_times:\n                avg_time = sum(self.query_times) / len(self.query_times) * 1000\n                p95_time = sorted(self.query_times)[int(len(self.query_times) * 0.95)] * 1000\n                max_time = max(self.query_times) * 1000\n            else:\n                avg_time = p95_time = max_time = 0\n            \n            print("\\n\\nLoad Test Results:")\n            print(f"Duration: {actual_duration:.2f}s")\n            print(f"Queries completed: {self.queries_completed}")\n            print(f"Queries per second: {qps:.2f}")\n            print(f"Errors: {self.errors}")\n            print(f"Avg query time: {avg_time:.2f}ms")\n            print(f"P95 query time: {p95_time:.2f}ms")\n            print(f"Max query time: {max_time:.2f}ms")\n\n# Example usage\nindex = Index(in_memory=True)\n# ... add documents to the index ...\n\n# List of test queries\ntest_queries = [\n    "common", "term", "query", "search", "index",\n    "document", "content", "text", "word", "language"\n]\n\n# Run the load test\nload_test = LoadTest(index, duration=30, threads=4)\nload_test.run(test_queries)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"mocking-for-tests",children:"Mocking for Tests"}),"\n",(0,r.jsx)(n.p,{children:"When testing code that uses the Fast-Inverted-Index library, you might want to mock it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# mock_index.py\nclass MockIndex:\n    """Mock implementation for testing."""\n    \n    def __init__(self, *args, **kwargs):\n        self.documents = {}  # doc_id -> content\n        self.metadata = {}   # doc_id -> metadata\n        self.removed = set() # removed doc_ids\n    \n    def add_document(self, doc_id, content, metadata=None):\n        """Mock adding a document."""\n        self.documents[doc_id] = content\n        if metadata:\n            self.metadata[doc_id] = metadata\n    \n    def remove_document(self, doc_id):\n        """Mock removing a document."""\n        if doc_id in self.documents:\n            del self.documents[doc_id]\n            self.removed.add(doc_id)\n            if doc_id in self.metadata:\n                del self.metadata[doc_id]\n    \n    def term_query(self, term):\n        """Mock term query."""\n        term = term.lower()\n        return [doc_id for doc_id, content in self.documents.items() \n                if term in content.lower()]\n    \n    def and_query(self, terms):\n        """Mock AND query."""\n        if not terms:\n            return []\n        \n        # Start with all documents matching the first term\n        result_ids = set(self.term_query(terms[0]))\n        \n        # Intersect with documents matching each additional term\n        for term in terms[1:]:\n            term_ids = set(self.term_query(term))\n            result_ids &= term_ids\n        \n        return list(result_ids)\n    \n    def or_query(self, terms):\n        """Mock OR query."""\n        result_ids = set()\n        \n        for term in terms:\n            term_ids = set(self.term_query(term))\n            result_ids |= term_ids\n        \n        return list(result_ids)\n    \n    def parse_query(self, query_str):\n        """Mock parse query."""\n        terms = query_str.lower().split()\n        return self.and_query(terms)\n    \n    def suggest_terms(self, prefix):\n        """Mock term suggestions."""\n        prefix = prefix.lower()\n        \n        # Extract all words from all documents\n        all_words = set()\n        for content in self.documents.values():\n            words = content.lower().split()\n            all_words.update(words)\n        \n        # Filter by prefix\n        matching_words = [w for w in all_words if w.startswith(prefix)]\n        \n        # Return in required format\n        return [\n            {"term": word, "score": 1.0, "doc_frequency": 1}\n            for word in matching_words[:10]  # Limit to 10 suggestions\n        ]\n    \n    def get_document(self, doc_id):\n        """Mock get document."""\n        if doc_id not in self.documents:\n            raise KeyError(f"Document not found: {doc_id}")\n        \n        result = {"id": doc_id, "content": self.documents[doc_id]}\n        \n        if doc_id in self.metadata:\n            result.update(self.metadata[doc_id])\n        \n        return result\n    \n    def stats(self):\n        """Mock index statistics."""\n        return {\n            "document_count": len(self.documents),\n            "term_count": len(set(w for content in self.documents.values() \n                              for w in content.split())),\n            "token_count": sum(len(content.split()) for content in self.documents.values()),\n            "avg_doc_length": sum(len(content.split()) for content in self.documents.values()) / \n                             max(1, len(self.documents)),\n            "cache_hit_rate": 0.0,\n            "avg_query_time_ms": 0.1,\n            "avg_indexing_time_ms": 0.1\n        }\n    \n    def optimize(self):\n        """Mock optimize operation."""\n        pass\n    \n    def backup(self, path):\n        """Mock backup operation."""\n        pass\n'})}),"\n",(0,r.jsx)(n.p,{children:"Using the mock in tests:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import unittest\nfrom unittest.mock import patch\nfrom your_application import SearchEngine\nfrom mock_index import MockIndex\n\nclass TestSearchEngine(unittest.TestCase):\n    @patch(\'fast_inverted_index.Index\', MockIndex)\n    def test_search_functionality(self):\n        """Test search engine with a mock index."""\n        search_engine = SearchEngine()\n        \n        # Add some test documents\n        search_engine.add_document("Document 1 with test content")\n        search_engine.add_document("Document 2 with different content")\n        \n        # Test search\n        results = search_engine.search("test")\n        self.assertEqual(len(results), 1)\n        \n        # Test suggestions\n        suggestions = search_engine.suggest("doc")\n        self.assertTrue(any(s["term"] == "document" for s in suggestions))\n'})}),"\n",(0,r.jsx)(n.h2,{id:"continuous-integration",children:"Continuous Integration"}),"\n",(0,r.jsx)(n.h3,{id:"automated-testing",children:"Automated Testing"}),"\n",(0,r.jsx)(n.p,{children:"Set up automated testing in a CI pipeline using a tool like GitHub Actions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# .github/workflows/test.yml\nname: Test\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main, develop ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.7, 3.8, 3.9, "3.10"]\n    \n    steps:\n    - uses: actions/checkout@v2\n    \n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v2\n      with:\n        python-version: ${{ matrix.python-version }}\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install pytest pytest-cov\n        pip install -e .\n    \n    - name: Run tests\n      run: |\n        pytest --cov=fast_inverted_index tests/\n    \n    - name: Upload coverage report\n      uses: codecov/codecov-action@v1\n'})}),"\n",(0,r.jsx)(n.h3,{id:"performance-regression-testing",children:"Performance Regression Testing"}),"\n",(0,r.jsx)(n.p,{children:"Set up automated performance regression tests:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# .github/workflows/benchmark.yml\nname: Performance Benchmark\n\non:\n  push:\n    branches: [ main ]\n  schedule:\n    - cron: '0 0 * * 0'  # Run weekly on Sundays at midnight\n\njobs:\n  benchmark:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v2\n    \n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.9'\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -e .\n        pip install pytest pytest-benchmark\n    \n    - name: Run benchmarks\n      run: |\n        pytest benchmarks/ --benchmark-json=benchmark_results.json\n    \n    - name: Compare with previous benchmarks\n      uses: benchmark-action/github-action-benchmark@v1\n      with:\n        tool: 'pytest'\n        output-file-path: benchmark_results.json\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n        auto-push: true\n        comment-on-alert: true\n        fail-on-alert: true\n        alert-threshold: '200%'  # Alert if performance drops by 2x\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isolate Test Environment"}),": Use in-memory mode for tests to avoid file system dependencies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Representative Data"}),": Test with data that resembles your production workload."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Boundary Testing"}),": Test edge cases like empty indices, large documents, etc."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Error Handling"}),": Verify error conditions and exception handling."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Awareness"}),": Include performance expectations in tests (with reasonable thresholds)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Test Suite Organization"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Unit tests for focused functionality"}),"\n",(0,r.jsx)(n.li,{children:"Integration tests for component interactions"}),"\n",(0,r.jsx)(n.li,{children:"End-to-end tests for complete workflows"}),"\n",(0,r.jsx)(n.li,{children:"Performance tests for benchmarking"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Continuous Testing"}),": Run tests automatically on code changes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Documentation"}),": Keep test documentation up-to-date with code changes."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"By following these testing practices, you can ensure the reliability and performance of your Fast-Inverted-Index implementation."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}}}]);